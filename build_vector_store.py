import hashlib
import sys
import torch
from langchain.text_splitter import RecursiveCharacterTextSplitter  # å¯¼å…¥æ–‡æ¡£åˆ†å‰²å·¥å…·
from langchain_community.vectorstores.utils import DistanceStrategy
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings  # å¯¼å…¥HuggingFaceåµŒå…¥æ¨¡å‹
from langchain_community.vectorstores import FAISS  # å¯¼å…¥FAISSç”¨äºæ„å»ºå‘é‡æ•°æ®åº“
from langchain_community.document_loaders import UnstructuredPDFLoader  # æ–°å¢å¯¼å…¥
from langchain_community.document_loaders import UnstructuredWordDocumentLoader
import os
import json
from pathlib import Path  # å¯¼å…¥Pathï¼Œç”¨äºè·¯å¾„å¤„ç†
from datetime import datetime  # å¯¼å…¥datetimeï¼Œç”¨äºè®°å½•æ—¶é—´æˆ³
from typing import List, Dict, Optional, Set, Tuple  # å¯¼å…¥ç±»å‹æç¤º
import logging  # å¯¼å…¥æ—¥å¿—æ¨¡å—ï¼Œç”¨äºè®°å½•è¿è¡Œæ—¥å¿—
from concurrent.futures import ThreadPoolExecutor, as_completed  # å¯¼å…¥çº¿ç¨‹æ± æ¨¡å—ï¼Œæ”¯æŒå¹¶è¡ŒåŠ è½½PDFæ–‡ä»¶
from tqdm import tqdm  # å¯¼å…¥è¿›åº¦æ¡æ¨¡å—ï¼Œç”¨äºæ˜¾ç¤ºåŠ è½½è¿›åº¦
from config import Config  # å¯¼å…¥é…ç½®ç±»ï¼Œç”¨äºåŠ è½½é…ç½®å‚æ•°
import shutil  # ç”¨äºæ–‡ä»¶æ“ä½œ

from pdf_cor_extractor.pdf_ocr_extractor import PDFProcessor


# é…ç½®æ—¥å¿—æ ¼å¼
# é…ç½®æ—¥å¿—æ ¼å¼ï¼ŒæŒ‡å®šè¾“å‡ºåˆ°stdout
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(stream=sys.stdout)],  # æ˜ç¡®è¾“å‡ºåˆ°stdout
    force=True  # å…³é”®ï¼šå¼ºåˆ¶è¦†ç›–ç°æœ‰é…ç½®
)
logger = logging.getLogger(__name__)


class VectorDBBuilder:
    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–å‘é‡æ•°æ®åº“æ„å»ºå™¨
        Args:
            config (Config): é…ç½®ç±»ï¼ŒåŒ…å«å¿…è¦çš„é…ç½®
        """
        self.config = config
        
        # è®¾ç½®ç¼“å­˜ç›®å½•è·¯å¾„
        self.cache_dir = Path(config.cache_dir)
        
        # è®¾ç½®å‘é‡æ•°æ®åº“è·¯å¾„
        self.vector_dir = Path(config.vector_db_path)
        self.vector_backup_dir = self.vector_dir / "backups"
        
        # æ–‡æœ¬å—ç¼“å­˜è·¯å¾„
        self.chunk_cache_path = self.cache_dir / "chunks_cache.json"
        
        # æ·»åŠ å¤„ç†çŠ¶æ€æ–‡ä»¶è·¯å¾„å®šä¹‰
        self.state_file = self.cache_dir / "processing_state.json"
        
        # å°†æºæ–‡ä»¶ç›®å½•å®šä¹‰æ”¾åœ¨åˆå§‹åŒ–æ–¹æ³•ä¸­
        self.subfolders = ['æ ‡å‡†']  # 'æ ‡å‡†æ€§æ–‡ä»¶','æ³•å¾‹', 'è§„èŒƒæ€§æ–‡ä»¶'
        
        # æ£€æŸ¥æ–‡ä»¶åŒ¹é…æ¨¡å¼
        if not hasattr(config, 'files') or not config.files:
            # å¦‚æœconfigä¸­æ²¡æœ‰fileså‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼
            self.config.files = ["data/**/*.pdf", "data/**/*.txt", "data/**/*.md", "data/**/*.docx"]
        
        # æ·»åŠ GPUä½¿ç”¨é…ç½®
        self.use_gpu_for_ocr = "cuda" in self.config.device
        
        # å·²å¤„ç†æ–‡ä»¶çŠ¶æ€
        self.processed_files = {}
        self.failed_files_count = 0
        self.need_rebuild_index = False
        
        # æ˜¯å¦è¾“å‡ºè¯¦ç»†çš„åˆ†å—å†…å®¹
        self.print_detailed_chunks = getattr(config, 'print_detailed_chunks', False)
        # è¯¦ç»†è¾“å‡ºæ—¶æ¯ä¸ªæ–‡æœ¬å—æ˜¾ç¤ºçš„æœ€å¤§å­—ç¬¦æ•°
        self.max_chunk_preview_length = getattr(config, 'max_chunk_preview_length', 200)

    def _load_processing_state(self) -> Dict:
        """åŠ è½½ä¹‹å‰çš„æ–‡ä»¶å¤„ç†çŠ¶æ€"""
        if self.state_file.exists():  # å¦‚æœçŠ¶æ€æ–‡ä»¶å­˜åœ¨ï¼Œåˆ™åŠ è½½
            with open(self.state_file, "r", encoding='utf-8') as f:
                return json.load(f)  # è¿”å›åŠ è½½çš„çŠ¶æ€æ–‡ä»¶å†…å®¹
        return {}  # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºå­—å…¸

    def _save_processing_state(self):
        """ä¿å­˜å½“å‰æ–‡ä»¶å¤„ç†çŠ¶æ€"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            self.state_file.parent.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜çŠ¶æ€
            with open(self.state_file, "w", encoding='utf-8') as f:
                json.dump(self.processed_files, f, indent=2, ensure_ascii=False)
                
            logger.debug(f"âœ… å·²ä¿å­˜å¤„ç†çŠ¶æ€ï¼Œå…± {len(self.processed_files)} ä¸ªæ–‡ä»¶è®°å½•")
        except Exception as e:
            logger.warning(f"âš ï¸ ä¿å­˜å¤„ç†çŠ¶æ€å¤±è´¥: {str(e)}")
            # è¿™é‡Œä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…ä¸­æ–­ä¸»æµç¨‹

    def _should_process(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦éœ€è¦å¤„ç†"""
        file_stat = file_path.stat()  # è·å–æ–‡ä»¶çŠ¶æ€
        file_info = {
            "path": str(file_path),  # æ–‡ä»¶è·¯å¾„
            "size": file_stat.st_size,  # æ–‡ä»¶å¤§å°
            "mtime": file_stat.st_mtime  # æ–‡ä»¶ä¿®æ”¹æ—¶é—´
        }

        existing = self.processed_files.get(str(file_path))  # æŸ¥æ‰¾å·²å¤„ç†çš„æ–‡ä»¶è®°å½•
        if not existing:  # å¦‚æœæ–‡ä»¶æœªå¤„ç†è¿‡ï¼Œåˆ™éœ€è¦å¤„ç†
            return True
        return not (existing["size"] == file_info["size"]
                    and existing["mtime"] == file_info["mtime"])  # å¦‚æœæ–‡ä»¶ä¿®æ”¹æ—¶é—´æˆ–å¤§å°ä¸ä¸€è‡´ï¼Œåˆ™éœ€è¦é‡æ–°å¤„ç†

    def _check_file_changes(self, current_files: List[Path]) -> Tuple[Set[str], Set[str], Set[str]]:
        """
        æ£€æŸ¥æ–‡ä»¶å˜åŒ–ï¼Œè¿”å›éœ€è¦æ·»åŠ ã€æ›´æ–°å’Œåˆ é™¤çš„æ–‡ä»¶
        
        Args:
            current_files: å½“å‰æ–‡ä»¶ç³»ç»Ÿä¸­çš„æ–‡ä»¶åˆ—è¡¨
            
        Returns:
            Tuple[Set[str], Set[str], Set[str]]: æ–°å¢æ–‡ä»¶ã€æ›´æ–°æ–‡ä»¶å’Œåˆ é™¤æ–‡ä»¶çš„è·¯å¾„é›†åˆ
        """
        # è·å–å½“å‰æ–‡ä»¶ç³»ç»Ÿä¸­çš„æ‰€æœ‰æ–‡ä»¶è·¯å¾„
        current_file_paths = {str(f) for f in current_files}
        
        # è·å–å·²å¤„ç†æ–‡ä»¶çš„è·¯å¾„
        processed_file_paths = set(self.processed_files.keys())
        
        # éœ€è¦æ–°å¢çš„æ–‡ä»¶ï¼ˆå½“å‰å­˜åœ¨ä½†æœªå¤„ç†è¿‡ï¼‰
        new_files = current_file_paths - processed_file_paths
        
        # éœ€è¦æ›´æ–°çš„æ–‡ä»¶ï¼ˆå·²å¤„ç†è¿‡ä½†éœ€è¦é‡æ–°å¤„ç†ï¼‰
        update_files = {str(f) for f in current_files if self._should_process(f)}
        
        # éœ€è¦åˆ é™¤çš„æ–‡ä»¶ï¼ˆå·²å¤„ç†è¿‡ä½†å½“å‰ä¸å­˜åœ¨ï¼‰
        deleted_files = processed_file_paths - current_file_paths
        
        return new_files, update_files, deleted_files

    def _load_single_document(self, file_path: Path) -> Optional[List[Document]]:
        """å¤šçº¿ç¨‹åŠ è½½å•ä¸ªæ–‡æ¡£æ–‡ä»¶ï¼ˆæ”¯æŒ PDFã€DOCXã€DOCï¼‰"""
        try:
            if not self._should_process(file_path):
                logger.info(f"[æ–‡æ¡£åŠ è½½] è·³è¿‡æœªä¿®æ”¹æ–‡ä»¶: {file_path.name}")
                return None

            file_extension = file_path.suffix.lower()
            docs = []

            if file_extension == ".pdf":
                try:
                    # æ£€æŸ¥PDFé¡µæ•°
                    import fitz
                    with fitz.open(str(file_path)) as doc:
                        page_count = doc.page_count
                        logger.info(f"[æ–‡æ¡£åŠ è½½] PDFæ–‡ä»¶ '{file_path.name}' å…±æœ‰ {page_count} é¡µ")
                        
                    # ä½¿ç”¨é…ç½®ä¸­çš„å‚æ•°åˆå§‹åŒ–å¤„ç†å™¨
                    processor = PDFProcessor(
                        file_path=str(file_path), 
                        lang='ch', 
                        use_gpu=self.use_gpu_for_ocr
                    )
                    
                    # æ ¹æ®é¡µæ•°é€‰æ‹©åˆé€‚çš„GPUå‚æ•°é…ç½®
                    if page_count > 30:
                        logger.info(f"[æ–‡æ¡£åŠ è½½] PDFé¡µæ•°è¾ƒå¤š({page_count}é¡µ)ï¼Œåº”ç”¨å¤§æ–‡æ¡£ä¼˜åŒ–é…ç½®")
                        processor.configure_gpu(**self.config.pdf_ocr_large_doc_params)
                    else:
                        # ä½¿ç”¨æ ‡å‡†å‚æ•°é…ç½®
                        processor.configure_gpu(**self.config.pdf_ocr_params)
                    
                    # å¤„ç†PDF
                    docs = processor.process()
                    
                    # æ£€æŸ¥å¤„ç†ç»“æœ
                    if docs and len(docs) < page_count * 0.5:
                        logger.warning(f"[æ–‡æ¡£åŠ è½½] è­¦å‘Š: åªè¯†åˆ«å‡º {len(docs)}/{page_count} é¡µï¼Œä½äº50%ï¼Œå¯èƒ½æœ‰é—®é¢˜")
                    elif docs:
                        logger.info(f"[æ–‡æ¡£åŠ è½½] æˆåŠŸè¯†åˆ« {len(docs)}/{page_count} é¡µ")
                    
                    # å¤„ç†åæ¸…ç†å†…å­˜
                    import gc
                    gc.collect()
                    try:
                        import torch
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                    except:
                        pass
                        
                except Exception as e:
                    logger.error(f"[æ–‡æ¡£åŠ è½½] å¤„ç†PDFæ–‡ä»¶ '{file_path.name}' å¤±è´¥: {str(e)}")
                    self.failed_files_count += 1
                    return None
                    
            elif file_extension in [".docx", ".doc"]:
                try:
                    # é¦–å…ˆå°è¯•å¯¼å…¥ä¾èµ–æ¨¡å—
                    try:
                        import docx2txt
                    except ImportError:
                        logger.error(f"ç¼ºå°‘å¤„ç†Wordæ–‡æ¡£æ‰€éœ€çš„ä¾èµ–åŒ…ï¼Œè¯·è¿è¡Œ: pip install docx2txt")
                        # è®°å½•é”™è¯¯ä½†ç»§ç»­æ‰§è¡Œï¼Œä»¥ä¾¿å¤„ç†å…¶ä»–æ–‡ä»¶ç±»å‹
                        self.failed_files_count += 1
                        return None
                        
                    from langchain_community.document_loaders import Docx2txtLoader
                    loader = Docx2txtLoader(str(file_path))
                    docs = loader.load()
                except Exception as e:
                    logger.error(f"[æ–‡æ¡£åŠ è½½] å¤„ç†DOCXæ–‡ä»¶ '{file_path.name}' å¤±è´¥: {str(e)}")
                    
                    # å°è¯•ä½¿ç”¨æ›¿ä»£æ–¹æ³•
                    try:
                        logger.info(f"[æ–‡æ¡£åŠ è½½] å°è¯•ä½¿ç”¨æ›¿ä»£æ–¹æ³•åŠ è½½Wordæ–‡æ¡£...")
                        from langchain_community.document_loaders import UnstructuredWordDocumentLoader
                        loader = UnstructuredWordDocumentLoader(str(file_path))
                        docs = loader.load()
                        logger.info(f"[æ–‡æ¡£åŠ è½½] æˆåŠŸä½¿ç”¨æ›¿ä»£æ–¹æ³•åŠ è½½Wordæ–‡æ¡£: {file_path.name}")
                    except Exception as e2:
                        logger.error(f"[æ–‡æ¡£åŠ è½½] æ›¿ä»£æ–¹æ³•ä¹Ÿå¤±è´¥: {str(e2)}")
                        self.failed_files_count += 1
                        return None
            else:
                logger.warning(f"[æ–‡æ¡£åŠ è½½] ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.name}")
                return None

            if docs:
                # è®°å½•æ–‡ä»¶å¤„ç†ä¿¡æ¯
                self.processed_files[str(file_path)] = {
                    "size": file_path.stat().st_size,
                    "mtime": file_path.stat().st_mtime,
                    "processed_at": datetime.now().isoformat(),
                    "pages": len(docs),
                    "file_name": file_path.name  # æ·»åŠ æ–‡ä»¶åï¼Œä¾¿äºåç»­æŸ¥æ‰¾
                }
                # ç»Ÿä¸€æ·»åŠ å…ƒæ•°æ®
                for doc in docs:
                    doc.metadata["source"] = str(file_path)
                    doc.metadata["file_name"] = file_path.name
                return docs
            return None

        except Exception as e:
            logger.error(f"[æ–‡æ¡£åŠ è½½] åŠ è½½ {file_path} å¤±è´¥: {str(e)}")
            self.failed_files_count += 1
            return None

    def _cleanup_deleted_files(self):
        """
        æ¸…ç†å·²åˆ é™¤æ–‡ä»¶çš„ç¼“å­˜ä¿¡æ¯
        """
        if not self.chunk_cache_path.exists():
            return
            
        try:
            # è·å–å½“å‰æ‰€æœ‰æ–‡ä»¶è·¯å¾„
            current_files = []
            # ä½¿ç”¨self.subfoldersä»£æ›¿ç¡¬ç¼–ç çš„å­æ–‡ä»¶å¤¹åˆ—è¡¨
            for subfolder in self.subfolders:
                folder_path = self.config.data_dir / subfolder
                if folder_path.exists() and folder_path.is_dir():
                    current_files.extend([f for f in folder_path.rglob("*") 
                                       if f.suffix.lower() in ['.pdf', '.docx', '.doc']])
                    
            # åˆ†ææ–‡ä»¶å˜åŒ–
            new_files, update_files, deleted_files = self._check_file_changes(current_files)
            
            if not deleted_files and not update_files:
                logger.info("æ²¡æœ‰æ£€æµ‹åˆ°æ–‡ä»¶å˜åŒ–ï¼Œè·³è¿‡æ¸…ç†")
                return
                
            # å¦‚æœæœ‰æ–‡ä»¶è¢«åˆ é™¤æˆ–æ›´æ–°ï¼Œéœ€è¦æ¸…ç†ç¼“å­˜
            with open(self.chunk_cache_path, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
                
            # æ¸…ç†å·²åˆ é™¤æ–‡ä»¶å’Œæ›´æ–°æ–‡ä»¶çš„æ•°æ®
            files_to_clean = deleted_files.union(update_files)
            if files_to_clean:
                logger.info(f"æ£€æµ‹åˆ° {len(deleted_files)} ä¸ªå·²åˆ é™¤æ–‡ä»¶, {len(update_files)} ä¸ªå·²æ›´æ–°æ–‡ä»¶")
                
                # ä»å¤„ç†çŠ¶æ€ä¸­åˆ é™¤å·²åˆ é™¤çš„æ–‡ä»¶
                for file_path in deleted_files:
                    if file_path in self.processed_files:
                        del self.processed_files[file_path]
                        logger.info(f"ä»å¤„ç†çŠ¶æ€ä¸­ç§»é™¤å·²åˆ é™¤æ–‡ä»¶: {Path(file_path).name}")
                
                # ä»ç¼“å­˜ä¸­è¿‡æ»¤æ‰å·²åˆ é™¤å’Œå·²æ›´æ–°çš„æ–‡ä»¶å—
                clean_chunks = []
                removed_count = 0
                
                for chunk in cache_data.get("chunks", []):
                    chunk_source = chunk.get("metadata", {}).get("source", "")
                    if chunk_source in files_to_clean:
                        removed_count += 1
                        continue
                    clean_chunks.append(chunk)
                
                # æ›´æ–°ç¼“å­˜æ•°æ®
                if removed_count > 0:
                    cache_data["chunks"] = clean_chunks
                    cache_data["file_state"] = self.processed_files
                    with open(self.chunk_cache_path, 'w', encoding='utf-8') as f:
                        json.dump(cache_data, f, ensure_ascii=False, indent=2)
                    logger.info(f"âœ… å·²ä»ç¼“å­˜ä¸­ç§»é™¤ {removed_count} ä¸ªè¿‡æ—¶çš„æ–‡æœ¬å—")
                    self.need_rebuild_index = True
                    
                    # ä¿å­˜æ›´æ–°åçš„å¤„ç†çŠ¶æ€
                    self._save_processing_state()
                    
        except Exception as e:
            logger.error(f"æ¸…ç†ç¼“å­˜å¤±è´¥: {str(e)}")
            # é‡åˆ°é”™è¯¯ï¼Œä¸ºå®‰å…¨èµ·è§æ ‡è®°éœ€è¦é‡å»ºç´¢å¼•
            self.need_rebuild_index = True

    def load_documents(self) -> List:
        """å¹¶è¡ŒåŠ è½½æ‰€æœ‰æ–‡æ¡£"""
        logger.info("âŒ› å¼€å§‹åŠ è½½æ–‡æ¡£...")
        
        # é¦–å…ˆæ¸…ç†å·²åˆ é™¤æ–‡ä»¶çš„ç¼“å­˜
        self._cleanup_deleted_files()

        # è·å– data_dir ä¸‹çš„æ‰€æœ‰å­æ–‡ä»¶å¤¹
        # ä½¿ç”¨self.subfoldersä»£æ›¿ç¡¬ç¼–ç çš„å­æ–‡ä»¶å¤¹åˆ—è¡¨
        document_files = []

        # éå†æ¯ä¸ªå­æ–‡ä»¶å¤¹ï¼Œè·å–å…¶ä¸­çš„æ–‡æ¡£æ–‡ä»¶
        for subfolder in self.subfolders:
            folder_path = self.config.data_dir / subfolder
            if folder_path.exists() and folder_path.is_dir():
                document_files.extend([f for f in folder_path.rglob("*") 
                                    if f.suffix.lower() in ['.pdf', '.docx', '.doc']])
            else:
                logger.warning(f"å­æ–‡ä»¶å¤¹ {subfolder} ä¸å­˜åœ¨æˆ–ä¸æ˜¯ç›®å½•: {folder_path}")
                
        # è¿‡æ»¤å¹¶æ’åºæ–‡ä»¶ï¼ˆå…ˆå¤„ç†è¾ƒå°çš„æ–‡ä»¶ï¼Œé¿å…å¤§æ–‡ä»¶å ç”¨æ˜¾å­˜ï¼‰
        document_files = sorted(document_files, key=lambda x: x.stat().st_size)
        logger.info(f"å‘ç° {len(document_files)} ä¸ªå¾…å¤„ç†æ–‡ä»¶")
        
        # è¿‡æ»¤å‡ºéœ€è¦å¤„ç†çš„æ–‡ä»¶
        files_to_process = [f for f in document_files if self._should_process(f)]
        logger.info(f"å…¶ä¸­ {len(files_to_process)} ä¸ªæ–‡ä»¶éœ€è¦å¤„ç†")
        
        if not files_to_process:
            logger.info("æ²¡æœ‰æ–°æ–‡ä»¶éœ€è¦å¤„ç†ï¼Œä½¿ç”¨ç°æœ‰ç¼“å­˜")
            # ä»ç¼“å­˜åŠ è½½æ–‡æ¡£
            return self._load_documents_from_cache()

        # é™åˆ¶çº¿ç¨‹æ± å¤§å°ä»¥é¿å…èµ„æºäº‰ç”¨
        with ThreadPoolExecutor(max_workers=1) as executor:
            futures = [executor.submit(self._load_single_document, file) for file in files_to_process]
            results = []
            with tqdm(total=len(futures), desc="åŠ è½½æ–‡æ¡£", unit="files") as pbar:
                for future in as_completed(futures):
                    res = future.result()
                    if res:
                        results.extend(res)
                        pbar.update(1)
                        pbar.set_postfix_str(f"å·²åŠ è½½ {len(res)} é¡µ")
                    else:
                        pbar.update(1)
        
        # åœ¨å¤„ç†å®Œæˆåæ¸…ç†GPUç¼“å­˜
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except:
            pass
            
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(results)} é¡µæ–‡æ¡£")
        logger.info(f"âŒ æœªæˆåŠŸåŠ è½½ {self.failed_files_count} ä¸ªæ–‡ä»¶")
        self._save_processing_state()
        
        # å¤„ç†æ–‡ä»¶å˜åŒ–åï¼Œå°†æ–°å¤„ç†çš„æ–‡æ¡£ä¸ç°æœ‰ç¼“å­˜åˆå¹¶
        if results and self.chunk_cache_path.exists():
            self._merge_with_cache(results)
            # è®¾ç½®æ ‡è®°ï¼Œè¡¨ç¤ºéœ€è¦é‡å»ºç´¢å¼•
            self.need_rebuild_index = True
            return self._load_documents_from_cache()
            
        return results

    def _load_documents_from_cache(self) -> List[Document]:
        """ä»ç¼“å­˜åŠ è½½æ–‡æ¡£"""
        try:
            if self.chunk_cache_path.exists():
                with open(self.chunk_cache_path, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                    chunks = [
                        Document(
                            page_content=chunk["content"],
                            metadata=chunk["metadata"]
                        )
                        for chunk in cache_data.get("chunks", [])
                    ]
                    logger.info(f"ä»ç¼“å­˜åŠ è½½äº† {len(chunks)} ä¸ªæ–‡æœ¬å—")
                    return chunks
        except Exception as e:
            logger.error(f"åŠ è½½ç¼“å­˜å¤±è´¥: {str(e)}")
        
        return []

    def _merge_with_cache(self, new_docs: List[Document]):
        """å°†æ–°å¤„ç†çš„æ–‡æ¡£ä¸ç°æœ‰ç¼“å­˜åˆå¹¶"""
        try:
            # åŠ è½½ç°æœ‰ç¼“å­˜
            if not self.chunk_cache_path.exists():
                logger.info("ç¼“å­˜ä¸å­˜åœ¨ï¼Œè·³è¿‡åˆå¹¶")
                return
                
            with open(self.chunk_cache_path, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
                
            # ä½¿ç”¨ä¸process_filesç›¸åŒçš„åˆ†å—é€»è¾‘å¤„ç†æ–°æ–‡æ¡£
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.config.chunk_size,
                chunk_overlap=self.config.chunk_overlap,
                separators=["\n\n", "\n", "ã€‚", "ï¼›", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""],
                length_function=len,
                add_start_index=True,
                is_separator_regex=False
            )
            
            # å¤„ç†æ–°æ–‡æ¡£
            new_chunks = []
            for doc in new_docs:
                metadata = doc.metadata.copy()
                split_texts = text_splitter.split_text(doc.page_content)
                for text in split_texts:
                    # ç”Ÿæˆå†…å®¹å“ˆå¸Œ
                    content_hash = hashlib.md5(text.encode()).hexdigest()
                    enhanced_metadata = metadata.copy()
                    enhanced_metadata["content_hash"] = content_hash
                    new_chunks.append({
                        "content": text,
                        "metadata": enhanced_metadata
                    })
                    
            # åˆå¹¶ç¼“å­˜
            existing_chunks = cache_data.get("chunks", [])
            
            # ä»ç°æœ‰ç¼“å­˜ä¸­è¿‡æ»¤æ‰ä¸æ–°æ–‡æ¡£ç›¸åŒæ¥æºçš„å—
            filtered_chunks = []
            new_sources = {doc.metadata.get("source") for doc in new_docs}
            for chunk in existing_chunks:
                chunk_source = chunk.get("metadata", {}).get("source", "")
                if chunk_source not in new_sources:
                    filtered_chunks.append(chunk)
                    
            # åˆå¹¶è¿‡æ»¤åçš„ç°æœ‰å—å’Œæ–°å—
            merged_chunks = filtered_chunks + new_chunks
            
            # æ›´æ–°ç¼“å­˜
            cache_data["chunks"] = merged_chunks
            cache_data["file_state"] = self.processed_files
            
            with open(self.chunk_cache_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
                
            logger.info(f"âœ… å·²å°† {len(new_chunks)} ä¸ªæ–°å—åˆå¹¶åˆ°ç¼“å­˜ä¸­ï¼Œæ€»è®¡ {len(merged_chunks)} ä¸ªå—")
            
        except Exception as e:
            logger.error(f"åˆå¹¶ç¼“å­˜å¤±è´¥: {str(e)}")

    def process_files(self) -> List:
        """ä¼˜åŒ–çš„æ–‡ä»¶å¤„ç†æµç¨‹"""
        logger.info("å¼€å§‹æ–‡ä»¶å¤„ç†æµç¨‹")
        
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ç°æœ‰ç¼“å­˜å¯ç”¨
        if self.chunk_cache_path.exists() and not self.need_rebuild_index:
            try:
                with open(self.chunk_cache_path, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                    
                    # æ¯”è¾ƒç¼“å­˜ä¸­çš„æ–‡ä»¶çŠ¶æ€ä¸å½“å‰å¤„ç†çŠ¶æ€
                    if cache_data.get("file_state", {}) == self.processed_files:
                        chunks = [
                            Document(
                                page_content=chunk["content"],
                                metadata=chunk["metadata"]
                            )
                            for chunk in cache_data.get("chunks", [])
                        ]
                        logger.info(f"âœ… ä»ç¼“å­˜åŠ è½½ {len(chunks)} ä¸ªåˆ†å—")
                        
                        # æ‰“å°åˆ†å—ç»“æœæ¦‚è§ˆ
                        self._print_chunks_summary(chunks)
                        
                        return chunks
            except Exception as e:
                logger.error(f"ç¼“å­˜åŠ è½½å¤±è´¥: {str(e)}")

        # å¦‚æœæ²¡æœ‰åˆé€‚çš„ç¼“å­˜ï¼Œå¤„ç†æ‰€æœ‰æ–‡æ¡£
        all_docs = self.load_documents()

        if not all_docs:
            logger.warning("æ²¡æœ‰å¯å¤„ç†çš„æ–‡ä»¶å†…å®¹")
            return []

        # åªæœ‰åœ¨æ²¡æœ‰ä»load_documentsä¸­ä½¿ç”¨ç¼“å­˜æ—¶æ‰éœ€è¦è¿›è¡Œåˆ†å—å¤„ç†
        if not self.need_rebuild_index:
            # é¦–å…ˆæŒ‰æ–‡ä»¶åˆå¹¶é¡µé¢å†…å®¹ï¼Œé¿å…è·¨é¡µåˆ†å—æ–­è£‚
            logger.info("åˆå¹¶æ–‡ä»¶é¡µé¢å†…å®¹ï¼Œå‡†å¤‡è¿›è¡Œæ•´ä½“åˆ†å—...")
            
            # æŒ‰æ–‡ä»¶åˆ†ç»„æ•´ç†æ–‡æ¡£
            file_docs = {}
            for doc in all_docs:
                source = doc.metadata.get("source", "")
                if source not in file_docs:
                    file_docs[source] = []
                file_docs[source].append(doc)
            
            # å¯¹æ¯ä¸ªæ–‡ä»¶çš„é¡µé¢è¿›è¡Œæ’åºå’Œåˆå¹¶
            whole_docs = []
            for source, docs in file_docs.items():
                # æŒ‰é¡µç æ’åº
                sorted_docs = sorted(docs, key=lambda x: x.metadata.get("page", 0))
                
                # åˆå¹¶æ–‡ä»¶æ‰€æœ‰é¡µé¢çš„å†…å®¹
                full_content = "\n\n".join([doc.page_content for doc in sorted_docs])
                
                # åˆ›å»ºå®Œæ•´æ–‡æ¡£å¯¹è±¡
                file_doc = Document(
                    page_content=full_content,
                    metadata={
                        "source": source,
                        "file_name": sorted_docs[0].metadata.get("file_name", ""),
                        "page_count": len(sorted_docs),
                        "is_merged_doc": True  # æ ‡è®°ä¸ºåˆå¹¶åçš„å®Œæ•´æ–‡æ¡£
                    }
                )
                whole_docs.append(file_doc)
                
            logger.info(f"å·²å°† {len(all_docs)} é¡µå†…å®¹åˆå¹¶ä¸º {len(whole_docs)} ä¸ªå®Œæ•´æ–‡æ¡£")
            
            # å¯¹åˆå¹¶åçš„å®Œæ•´æ–‡æ¡£è¿›è¡Œåˆ†å—
            # ä¼˜åŒ–åçš„æ–‡æœ¬åˆ†å‰²é…ç½®
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.config.chunk_size,  # æ ¹æ®ä¸­æ–‡å¹³å‡é•¿åº¦è°ƒæ•´
                chunk_overlap=self.config.chunk_overlap,  # é€‚å½“å¢åŠ é‡å é‡
                separators=[
                    "\n\n",  # ä¼˜å…ˆæŒ‰ç©ºè¡Œåˆ†å‰²
                    "\n",  # å…¶æ¬¡æŒ‰æ¢è¡Œç¬¦åˆ†å‰²
                    "ã€‚", "ï¼›",  # ä¸­æ–‡å¥æœ«æ ‡ç‚¹
                    "ï¼", "ï¼Ÿ",  # ä¸­æ–‡æ„Ÿå¹å·å’Œé—®å·
                    "ï¼Œ",  # ä¸­æ–‡é€—å·ï¼ˆè°¨æ…ä½¿ç”¨ï¼‰
                    " ",  # ç©ºæ ¼
                    ""  # æœ€åæŒ‰å­—ç¬¦åˆ†å‰²
                ],
                length_function=len,
                add_start_index=True,
                is_separator_regex=False  # æ˜ç¡®ä½¿ç”¨å­—é¢åˆ†éš”ç¬¦
            )

            logger.info("å¼€å§‹æ™ºèƒ½åˆ†å—å¤„ç†...")
            chunks = []
            # æ–°å¢å“ˆå¸Œç”Ÿæˆå’Œå…ƒæ•°æ®å¢å¼º
            with tqdm(total=len(whole_docs), desc="å¤„ç†æ–‡æ¡£åˆ†å—") as pbar:
                for doc in whole_docs:
                    metadata = doc.metadata.copy()
                    # ç§»é™¤åˆ†å—åä¸å†é€‚ç”¨çš„å…ƒæ•°æ®
                    if "is_merged_doc" in metadata:
                        del metadata["is_merged_doc"]
                    
                    # å¯¹å®Œæ•´æ–‡æ¡£è¿›è¡Œåˆ†å—
                    split_texts = text_splitter.split_text(doc.page_content)
                    
                    # å¤„ç†æ¯ä¸ªæ–‡æœ¬å—
                    for i, text in enumerate(split_texts):
                        # åº”ç”¨æ™ºèƒ½è¾¹ç•Œå¤„ç†ï¼Œç¡®ä¿å®Œæ•´å¥å­
                        text = self._ensure_complete_sentences(text)
                        if not text.strip():  # è·³è¿‡ç©ºæ–‡æœ¬å—
                            continue
                            
                        # ç”Ÿæˆå†…å®¹å“ˆå¸Œ
                        content_hash = hashlib.md5(text.encode()).hexdigest()
                        enhanced_metadata = metadata.copy()
                        enhanced_metadata["content_hash"] = content_hash
                        enhanced_metadata["chunk_index"] = i
                        enhanced_metadata["total_chunks"] = len(split_texts)
                        
                        # æ·»åŠ å—ä½ç½®æ ‡è®°
                        if i == 0:
                            enhanced_metadata["position"] = "document_start"
                        elif i == len(split_texts) - 1:
                            enhanced_metadata["position"] = "document_end"
                        else:
                            enhanced_metadata["position"] = "document_middle"

                        chunks.append(Document(
                            page_content=text,
                            metadata=enhanced_metadata
                        ))
                    pbar.update(1)
            
            # åº”ç”¨åå¤„ç†ï¼Œç¡®ä¿æ–‡æœ¬å—çš„å®Œæ•´æ€§å’Œè¿è´¯æ€§
            chunks = self._post_process_chunks(chunks)
            
            logger.info(f"ç”Ÿæˆ {len(chunks)} ä¸ªè¯­ä¹‰è¿è´¯çš„æ–‡æœ¬å—")
            
            # æ‰“å°åˆ†å—ç»“æœæ¦‚è§ˆ
            self._print_chunks_summary(chunks)

            # ä¿å­˜å¤„ç†ç»“æœåˆ°ç¼“å­˜
            cache_data = {
                "file_state": self.processed_files,
                "chunks": [{
                    "content": chunk.page_content,
                    "metadata": chunk.metadata
                } for chunk in chunks]
            }

            with open(self.chunk_cache_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)

            logger.info(f"âœ… åˆ†å—ç¼“å­˜å·²ä¿å­˜è‡³ {self.chunk_cache_path}")
            return chunks
        else:
            # å¦‚æœå·²ç»é€šè¿‡load_documentsåŠ è½½å¹¶æ›´æ–°äº†ç¼“å­˜ï¼Œç›´æ¥è¿”å›æ–‡æ¡£
            return all_docs
            
    def _ensure_complete_sentences(self, text: str) -> str:
        """ç¡®ä¿æ–‡æœ¬å—ä»¥å®Œæ•´å¥å­å¼€å§‹å’Œç»“æŸ
        
        Args:
            text: åŸå§‹æ–‡æœ¬å—
            
        Returns:
            å¤„ç†åçš„æ–‡æœ¬å—ï¼Œç¡®ä¿ä»¥å®Œæ•´å¥å­å¼€å§‹å’Œç»“æŸ
        """
        if not text or len(text) < 10:  # æ–‡æœ¬è¿‡çŸ­åˆ™ç›´æ¥è¿”å›
            return text
            
        # ä¸­æ–‡å¥å­ç»“æŸæ ‡è®°
        sentence_end_marks = ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', '\n']
        # å¥å­å¼€å§‹çš„å¯èƒ½æ ‡è®°ï¼ˆä¸­æ–‡æ®µè½å¼€å¤´ã€ç« èŠ‚æ ‡é¢˜ç­‰ï¼‰
        sentence_start_patterns = ['\n', 'ç¬¬.{1,3}ç« ', 'ç¬¬.{1,3}èŠ‚']
        
        # å¤„ç†æ–‡æœ¬å—å¼€å¤´
        text_stripped = text.lstrip()
        # å¦‚æœä¸æ˜¯ä»¥å¥æœ«æ ‡ç‚¹å¼€å¤´ï¼Œä¹Ÿä¸æ˜¯ä»¥å¤§å†™å­—æ¯æˆ–æ•°å­—å¼€å¤´ï¼ˆå¯èƒ½æ˜¯æ–°æ®µè½ï¼‰ï¼Œåˆ™å¯èƒ½æ˜¯ä¸å®Œæ•´å¥å­
        is_incomplete_start = True
        
        # æ£€æŸ¥æ˜¯å¦ä»¥å®Œæ•´å¥å­æˆ–æ®µè½å¼€å§‹çš„æ ‡è®°
        for pattern in sentence_start_patterns:
            if text.startswith(pattern) or text_stripped[0].isupper() or text_stripped[0].isdigit():
                is_incomplete_start = False
                break
        
        if is_incomplete_start:
            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå®Œæ•´å¥å­çš„å¼€å§‹
            for mark in sentence_end_marks:
                pos = text.find(mark)
                if pos > 0:
                    # æ‰¾åˆ°å¥æœ«æ ‡è®°åçš„å†…å®¹ä½œä¸ºèµ·ç‚¹
                    try:
                        # ç¡®ä¿å¥æœ«æ ‡è®°åè¿˜æœ‰å†…å®¹
                        if pos + 1 < len(text):
                            text = text[pos+1:].lstrip()
                            break
                    except:
                        # å‡ºé”™åˆ™ä¿æŒåŸæ ·
                        pass
        
        # å¤„ç†æ–‡æœ¬å—ç»“å°¾
        is_incomplete_end = True
        # æ£€æŸ¥æ˜¯å¦ä»¥å®Œæ•´å¥å­ç»“æŸ
        for mark in sentence_end_marks:
            if text.endswith(mark):
                is_incomplete_end = False
                break
        
        if is_incomplete_end:
            # æ‰¾æœ€åä¸€ä¸ªå®Œæ•´å¥å­çš„ç»“æŸä½ç½®
            last_pos = -1
            for mark in sentence_end_marks:
                pos = text.rfind(mark)
                if pos > last_pos:
                    last_pos = pos
                    
            if last_pos > 0:
                # æˆªå–åˆ°æœ€åä¸€ä¸ªå®Œæ•´å¥å­ç»“æŸ
                text = text[:last_pos+1]
        
        return text.strip()
    
    def _post_process_chunks(self, chunks: List[Document]) -> List[Document]:
        """å¯¹åˆ†å—åçš„æ–‡æœ¬è¿›è¡Œåå¤„ç†ï¼Œä¼˜åŒ–å—çš„è´¨é‡
        
        Args:
            chunks: åŸå§‹åˆ†å—åˆ—è¡¨
            
        Returns:
            å¤„ç†åçš„åˆ†å—åˆ—è¡¨
        """
        if not chunks:
            return []
            
        logger.info("å¯¹æ–‡æœ¬å—è¿›è¡Œåå¤„ç†ä¼˜åŒ–...")
        processed_chunks = []
        
        # æŒ‰æ–‡æ¡£æºåˆ†ç»„å¤„ç†
        doc_chunks = {}
        for chunk in chunks:
            source = chunk.metadata.get("source", "")
            if source not in doc_chunks:
                doc_chunks[source] = []
            doc_chunks[source].append(chunk)
        
        total_merged = 0
        
        # å¤„ç†æ¯ä¸ªæ–‡æ¡£çš„å—
        for source, source_chunks in doc_chunks.items():
            # æŒ‰å—ç´¢å¼•æ’åº
            sorted_chunks = sorted(source_chunks, 
                                   key=lambda x: x.metadata.get("chunk_index", 0))
            
            # æ£€æŸ¥å’Œå¤„ç†ç›¸é‚»å—
            for i, chunk in enumerate(sorted_chunks):
                # ç¡®ä¿å®Œæ•´å¥å­
                chunk.page_content = self._ensure_complete_sentences(chunk.page_content)
                
                # è·³è¿‡ç©ºå—
                if not chunk.page_content.strip():
                    continue
                
                # è¿‡æ»¤æ‰è¿‡çŸ­çš„å—ï¼ˆä¾‹å¦‚åªæœ‰å‡ ä¸ªå­—çš„å—ï¼‰
                if len(chunk.page_content) < 50:  # è®¾ç½®æœ€å°å—é•¿åº¦é˜ˆå€¼
                    continue
                
                # æ£€æŸ¥ä¸å‰ä¸€ä¸ªå—çš„é‡å åº¦
                if i > 0 and processed_chunks:
                    prev_chunk = processed_chunks[-1]
                    if prev_chunk.metadata.get("source") == source:
                        # è®¡ç®—é‡å åº¦
                        overlap_ratio = self._calculate_overlap_ratio(
                            prev_chunk.page_content, chunk.page_content)
                        
                        # å¦‚æœé‡å åº¦è¿‡é«˜ï¼ˆè¶…è¿‡70%ï¼‰ï¼Œè€ƒè™‘åˆå¹¶æˆ–è·³è¿‡
                        if overlap_ratio > 0.7:
                            # å¦‚æœå½“å‰å—æ¯”å‰ä¸€ä¸ªå—çŸ­ï¼Œè·³è¿‡å½“å‰å—
                            if len(chunk.page_content) <= len(prev_chunk.page_content):
                                continue
                            # å¦åˆ™ç”¨å½“å‰å—æ›¿æ¢å‰ä¸€ä¸ªå—
                            else:
                                processed_chunks[-1] = chunk
                                continue
                
                processed_chunks.append(chunk)
        
        logger.info(f"åå¤„ç†å®Œæˆï¼Œä¼˜åŒ–åçš„å—æ•°: {len(processed_chunks)}")
        return processed_chunks
        
    def _calculate_overlap_ratio(self, text1: str, text2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬ä¹‹é—´çš„é‡å æ¯”ä¾‹
        
        Args:
            text1: ç¬¬ä¸€ä¸ªæ–‡æœ¬
            text2: ç¬¬äºŒä¸ªæ–‡æœ¬
            
        Returns:
            é‡å æ¯”ä¾‹ï¼ˆ0-1ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼‰
        """
        # ä½¿ç”¨è¾ƒçŸ­æ–‡æœ¬çš„é•¿åº¦ä½œä¸ºåˆ†æ¯
        min_len = min(len(text1), len(text2))
        if min_len == 0:
            return 0.0
            
        # å¯»æ‰¾æœ€é•¿çš„å…±åŒå­ä¸²
        for i in range(min_len, 0, -1):
            if text1.endswith(text2[:i]):
                return i / min_len
            if text2.endswith(text1[:i]):
                return i / min_len
                
        return 0.0

    def _print_chunks_summary(self, chunks: List[Document]):
        """æ‰“å°æ–‡æœ¬åˆ†å—ç»“æœæ¦‚è§ˆ"""
        if not chunks:
            logger.info("æ²¡æœ‰æ–‡æœ¬å—å¯ä¾›æ˜¾ç¤º")
            return
            
        # ç»Ÿè®¡ä¿¡æ¯
        total_chunks = len(chunks)
        avg_chunk_length = sum(len(chunk.page_content) for chunk in chunks) / total_chunks
        files_count = len(set(chunk.metadata.get("source", "") for chunk in chunks))
        
        logger.info("\n" + "="*50)
        logger.info("ğŸ“Š æ–‡æœ¬åˆ†å—å¤„ç†æ¦‚è§ˆ")
        logger.info("="*50)
        logger.info(f"ğŸ“„ æ€»å—æ•°: {total_chunks}")
        logger.info(f"ğŸ“Š å¹³å‡å—é•¿åº¦: {avg_chunk_length:.1f} å­—ç¬¦")
        logger.info(f"ğŸ“‚ æ¶‰åŠæ–‡ä»¶æ•°: {files_count}")
        
        # æ–‡ä»¶çº§ç»Ÿè®¡
        file_chunks = {}
        for chunk in chunks:
            source = chunk.metadata.get("source", "æœªçŸ¥æ¥æº")
            if source not in file_chunks:
                file_chunks[source] = []
            file_chunks[source].append(chunk)
        
        logger.info("\nğŸ“‚ æ–‡ä»¶çº§åˆ†å—ç»Ÿè®¡:")
        for file_path, file_chunks_list in sorted(file_chunks.items(), key=lambda x: len(x[1]), reverse=True):
            file_name = Path(file_path).name if isinstance(file_path, str) else "æœªçŸ¥æ–‡ä»¶"
            logger.info(f"  â€¢ {file_name}: {len(file_chunks_list)} å—")
        
        
        # è¾“å‡ºè¯¦ç»†åˆ†å—å†…å®¹ (å¦‚æœå¼€å¯)
        if self.print_detailed_chunks:
            self._print_detailed_chunks(chunks)
            
        logger.info("="*50)

    def _print_detailed_chunks(self, chunks: List[Document]):
        """è¾“å‡ºè¯¦ç»†çš„åˆ†å—å†…å®¹"""
        logger.info("\n" + "="*50)
        logger.info("ğŸ“‘ è¯¦ç»†æ–‡æœ¬å—å†…å®¹")
        logger.info("="*50)
        
        # å°†åˆ†å—æŒ‰æ–‡ä»¶åˆ†ç»„
        file_chunks = {}
        for chunk in chunks:
            source = chunk.metadata.get("source", "æœªçŸ¥æ¥æº")
            if source not in file_chunks:
                file_chunks[source] = []
            file_chunks[source].append(chunk)
        
        # ä¸ºäº†æ›´æœ‰ç»„ç»‡åœ°è¾“å‡ºï¼Œå…ˆæŒ‰æ–‡ä»¶è¾“å‡º
        for file_path, file_chunks_list in sorted(file_chunks.items(), key=lambda x: len(x[1]), reverse=True):
            file_name = Path(file_path).name if isinstance(file_path, str) else "æœªçŸ¥æ–‡ä»¶"
            logger.info(f"\nğŸ“„ æ–‡ä»¶: {file_name} (å…±{len(file_chunks_list)}å—)")
            
            # è¾“å‡ºè¯¥æ–‡ä»¶çš„å‰3ä¸ªå—
            for i, chunk in enumerate(file_chunks_list[:3]):
                page_num = chunk.metadata.get("page", "æœªçŸ¥é¡µç ")
                chunk_size = len(chunk.page_content)
                
                # è·å–é¢„è§ˆå†…å®¹
                content_preview = chunk.page_content
                if len(content_preview) > self.max_chunk_preview_length:
                    content_preview = content_preview[:self.max_chunk_preview_length] + "..."
                
                # æ›¿æ¢æ¢è¡Œç¬¦ä»¥ä¾¿äºæ§åˆ¶å°æ˜¾ç¤º
                content_preview = content_preview.replace("\n", "\\n")
                
                logger.info(f"\n  å— {i+1}/{len(file_chunks_list[:3])} [ç¬¬{page_num}é¡µ, {chunk_size}å­—ç¬¦]:")
                logger.info(f"  {content_preview}")
            
            # å¦‚æœæ–‡ä»¶ä¸­çš„å—æ•°è¶…è¿‡3ä¸ªï¼Œæ˜¾ç¤ºçœç•¥ä¿¡æ¯
            if len(file_chunks_list) > 3:
                logger.info(f"  ... è¿˜æœ‰ {len(file_chunks_list) - 3} ä¸ªå—æœªæ˜¾ç¤º ...")
                
        # è¾“å‡ºä¿å­˜å®Œæ•´åˆ†å—å†…å®¹çš„æç¤º
        chunks_detail_file = self.cache_dir / "chunks_detail.txt"
        try:
            with open(chunks_detail_file, "w", encoding="utf-8") as f:
                for i, chunk in enumerate(chunks):
                    source = chunk.metadata.get("source", "æœªçŸ¥æ¥æº")
                    file_name = Path(source).name if isinstance(source, str) else "æœªçŸ¥æ–‡ä»¶"
                    page_num = chunk.metadata.get("page", "æœªçŸ¥é¡µç ")
                    
                    f.write(f"=== å— {i+1}/{len(chunks)} [{file_name} - ç¬¬{page_num}é¡µ] ===\n")
                    f.write(chunk.page_content)
                    f.write("\n\n")
            
            logger.info(f"\nâœ… æ‰€æœ‰æ–‡æœ¬å—çš„è¯¦ç»†å†…å®¹å·²ä¿å­˜è‡³: {chunks_detail_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜è¯¦ç»†å—å†…å®¹å¤±è´¥: {str(e)}")
        
        logger.info("="*50)

    def create_embeddings(self) -> HuggingFaceEmbeddings:
        """åˆ›å»ºåµŒå…¥æ¨¡å‹å®ä¾‹"""
        logger.info("åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
        return HuggingFaceEmbeddings(
            model_name=self.config.embedding_model_path,  # åµŒå…¥æ¨¡å‹çš„è·¯å¾„
            model_kwargs={"device": self.config.device},  # è®¾ç½®è®¾å¤‡ä¸ºCPUæˆ–GPU
            encode_kwargs={
                "batch_size": self.config.batch_size,  # æ‰¹å¤„ç†å¤§å°
                "normalize_embeddings": self.config.normalize_embeddings  # æ˜¯å¦å½’ä¸€åŒ–åµŒå…¥
            },
        )

    def backup_vector_db(self):
        """å¤‡ä»½ç°æœ‰å‘é‡æ•°æ®åº“"""
        vector_db_path = Path(self.config.vector_db_path)
        if not vector_db_path.exists():
            return False
            
        try:
            # åˆ›å»ºå¤‡ä»½ç›®å½•
            backup_dir = vector_db_path.parent / f"{vector_db_path.name}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            backup_dir.mkdir(parents=True, exist_ok=True)
            
            # å¤åˆ¶æ‰€æœ‰æ–‡ä»¶åˆ°å¤‡ä»½ç›®å½•
            for item in vector_db_path.glob('*'):
                if item.is_file():
                    shutil.copy2(item, backup_dir)
                elif item.is_dir():
                    shutil.copytree(item, backup_dir / item.name)
                    
            logger.info(f"âœ… å‘é‡æ•°æ®åº“å·²å¤‡ä»½è‡³ {backup_dir}")
            return True
        except Exception as e:
            logger.error(f"å¤‡ä»½å‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
            return False

    def build_vector_store(self):
        """æ„å»ºå‘é‡æ•°æ®åº“"""
        logger.info("å¼€å§‹æ„å»ºå‘é‡æ•°æ®åº“")

        # åˆ›å»ºå¿…è¦ç›®å½•
        Path(self.config.vector_db_path).mkdir(parents=True, exist_ok=True)

        # å¤„ç†æ–‡æ¡£
        chunks = self.process_files()  # å¤„ç†æ–‡æ¡£å¹¶åˆ†å—
        
        if not chunks:
            logger.warning("æ²¡æœ‰æ–‡æ¡£å—å¯ä»¥å¤„ç†ï¼Œè·³è¿‡å‘é‡å­˜å‚¨æ„å»º")
            return

        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»ºç´¢å¼•
        if self.need_rebuild_index:
            logger.info("æ£€æµ‹åˆ°æ–‡ä»¶å˜åŒ–ï¼Œéœ€è¦é‡å»ºå‘é‡ç´¢å¼•")
            if Path(self.config.vector_db_path).exists() and any(Path(self.config.vector_db_path).glob('*')):
                self.backup_vector_db()

        # ç”ŸæˆåµŒå…¥æ¨¡å‹
        embeddings = self.create_embeddings()

        # æ„å»ºå‘é‡å­˜å‚¨
        logger.info("ç”Ÿæˆå‘é‡...")
        # æ„å»ºå‘é‡å­˜å‚¨æ—¶æ˜¾å¼æŒ‡å®š
        vector_store = FAISS.from_documents(
            chunks,
            embeddings,
            distance_strategy=DistanceStrategy.COSINE  # æ˜ç¡®æŒ‡å®šä½™å¼¦ç›¸ä¼¼åº¦
        )

        # ä¿å­˜å‘é‡æ•°æ®åº“
        vector_store.save_local(str(self.config.vector_db_path))  # ä¿å­˜å‘é‡å­˜å‚¨åˆ°æŒ‡å®šè·¯å¾„
        logger.info(f"å‘é‡æ•°æ®åº“å·²ä¿å­˜è‡³ {self.config.vector_db_path}")  # è¾“å‡ºä¿å­˜è·¯å¾„


if __name__ == "__main__":
    try:
        # åˆå§‹åŒ–é…ç½®
        config = Config()
        
        # æ·»åŠ : è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œå…è®¸ç”¨æˆ·æŒ‡å®šæ˜¯å¦æ‰“å°è¯¦ç»†åˆ†å—å†…å®¹
        import argparse
        parser = argparse.ArgumentParser(description='æ„å»ºåŒ–å·¥å®‰å…¨é¢†åŸŸå‘é‡æ•°æ®åº“')
        parser.add_argument('--detailed-chunks', action='store_true', 
                           help='æ˜¯å¦è¾“å‡ºè¯¦ç»†çš„åˆ†å—å†…å®¹')
        parser.add_argument('--max-preview', type=int, default=510,
                           help='è¯¦ç»†è¾“å‡ºæ—¶æ¯ä¸ªæ–‡æœ¬å—æ˜¾ç¤ºçš„æœ€å¤§å­—ç¬¦æ•°')
        args = parser.parse_args()
        
        # æ›´æ–°é…ç½®
        if args.detailed_chunks:
            config.print_detailed_chunks = True
            config.max_chunk_preview_length = args.max_preview
            print(f"å°†è¾“å‡ºè¯¦ç»†åˆ†å—å†…å®¹ï¼Œæ¯å—æœ€å¤šæ˜¾ç¤º {args.max_preview} å­—ç¬¦")

        # æ„å»ºå‘é‡æ•°æ®åº“
        builder = VectorDBBuilder(config)
        builder.build_vector_store()

    except Exception as e:
        logger.exception("ç¨‹åºè¿è¡Œå‡ºé”™")  # è®°å½•ç¨‹åºå¼‚å¸¸
    finally:
        logger.info("ç¨‹åºè¿è¡Œç»“æŸ")  # ç¨‹åºç»“æŸæ—¥å¿—
