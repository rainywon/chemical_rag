import logging
from typing import Generator, Optional, Tuple, List
import torch
from transformers import (
    AutoModelForCausalLM,  # å¯¼å…¥è‡ªå›å½’è¯­è¨€æ¨¡å‹ç±»
    AutoTokenizer,         # å¯¼å…¥è‡ªåŠ¨åˆ†è¯å™¨ç±»
    StoppingCriteria,     # å¯¼å…¥åœæ­¢æ¡ä»¶ç±»
    StoppingCriteriaList  # å¯¼å…¥åœæ­¢æ¡ä»¶åˆ—è¡¨ç±»
)
from config import Config  # å¯¼å…¥è‡ªå®šä¹‰é…ç½®ç±»

# è®¾ç½®æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)

class QwenLLM:
    """å¤§è¯­è¨€æ¨¡å‹å°è£…ç±»ï¼Œæ”¯æŒåŒæ­¥ç”Ÿæˆå’Œæµå¼ç”Ÿæˆ"""

    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–QwenLLMå®ä¾‹ã€‚
        :param config: é…ç½®å¯¹è±¡ï¼ŒåŒ…å«äº†æ¨¡å‹çš„é…ç½®å‚æ•°
        """
        self.config = config  # é…ç½®ç±»å®ä¾‹
        self.device = config.device  # è·å–é…ç½®ä¸­çš„è®¾å¤‡ï¼ˆCPUæˆ–GPUï¼‰
        self._load_components()  # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ç­‰ç»„ä»¶
        logger.info("âœ… æ¨¡å‹ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")

    def _load_components(self) -> None:
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        try:
            logging.basicConfig(
                level=logging.INFO,
                format="%(asctime)s - %(levelname)s - %(message)s",  # é…ç½®æ—¥å¿—æ ¼å¼
                handlers=[logging.StreamHandler()]  # è¾“å‡ºæ—¥å¿—åˆ°æ§åˆ¶å°
            )
            logger.info("ğŸ”§ æ­£åœ¨åŠ è½½åˆ†è¯å™¨...")
            # åŠ è½½æ¨¡å‹æ‰€éœ€çš„åˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.llm_model_path,  # ä»é…ç½®ä¸­è·å–æ¨¡å‹è·¯å¾„
                trust_remote_code=True,  # å…è®¸ä»è¿œç¨‹ä¸‹è½½ä»£ç 
                use_fast=True  # ä½¿ç”¨å¿«é€Ÿåˆ†è¯å™¨ï¼ˆå¯ä»¥åŠ é€Ÿåˆ†è¯è¿‡ç¨‹ï¼‰
            )

            logger.info("ğŸš€ æ­£åœ¨åŠ è½½å¤§æ¨¡å‹...")
            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.llm_model_path,  # ä»é…ç½®ä¸­è·å–æ¨¡å‹è·¯å¾„
                device_map=self.config.device,  # å°†æ¨¡å‹åŠ è½½åˆ°é…ç½®æŒ‡å®šçš„è®¾å¤‡ä¸Šï¼ˆCPU/GPUï¼‰
                torch_dtype=self.config.torch_dtype,  # æ¨¡å‹æƒé‡çš„æ•°æ®ç±»å‹ï¼ˆå¦‚ float16, float32ï¼‰
                trust_remote_code=True,  # å…è®¸ä»è¿œç¨‹ä¸‹è½½ä»£ç 
                # attn_implementation="flash_attention_2" if self.config.use_flash_attn else None,
                # quantization_config=self.config.quantization_config,
            ).eval()  # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆå…³é—­ dropout ç­‰ï¼‰

            if self.config.compile_model:
                logger.info("âš¡ æ­£åœ¨ç¼–è¯‘æ¨¡å‹...")
                # ä½¿ç”¨ PyTorch ç¼–è¯‘å™¨ä¼˜åŒ–æ¨¡å‹ï¼ˆåŠ é€Ÿæ¨ç†è¿‡ç¨‹ï¼‰
                self.model = torch.compile(self.model)

        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            raise
    def _prepare_inputs(self, prompt: str) -> Tuple[torch.Tensor, torch.Tensor]:
        """é¢„å¤„ç†è¾“å…¥"""
        try:
            # ä½¿ç”¨åˆ†è¯å™¨å°†è¾“å…¥çš„æ–‡æœ¬ï¼ˆpromptï¼‰è½¬æ¢ä¸ºæ¨¡å‹èƒ½å¤Ÿæ¥å—çš„æ ¼å¼
            inputs = self.tokenizer(
                prompt,  # è¾“å…¥çš„æç¤ºæ–‡æœ¬
                return_tensors="pt",  # è¿”å›PyTorchå¼ é‡æ ¼å¼
                max_length=2048,  # è®¾ç½®æœ€å¤§è¾“å…¥é•¿åº¦ï¼Œé˜²æ­¢è¶…å‡ºæ¨¡å‹çš„æœ€å¤§é•¿åº¦é™åˆ¶
                truncation=True  # è¶…è¿‡æœ€å¤§é•¿åº¦æ—¶è¿›è¡Œæˆªæ–­
            ).to(self.model.device)  # å°†è¾“å…¥ç§»åŠ¨åˆ°ä¸æ¨¡å‹ç›¸åŒçš„è®¾å¤‡ä¸Š
            return inputs.input_ids, inputs.attention_mask  # è¿”å›è¾“å…¥çš„token id å’Œ attention mask

        except Exception as e:
            logger.error(f"è¾“å…¥å¤„ç†å¤±è´¥: {str(e)}")  # å¦‚æœå‡ºç°é”™è¯¯ï¼Œè®°å½•æ—¥å¿—å¹¶æŠ›å‡ºå¼‚å¸¸
            raise

    def generate(self, prompt: str) -> str:
        """åŒæ­¥ç”Ÿæˆå®Œæ•´å›ç­”"""
        try:
            logger.info("ğŸ§  å¼€å§‹åŒæ­¥ç”Ÿæˆ...")
            input_ids, attention_mask = self._prepare_inputs(prompt)  # è·å–è¾“å…¥çš„token id å’Œ attention mask

            # è°ƒç”¨æ¨¡å‹çš„ generate æ–¹æ³•è¿›è¡Œæ–‡æœ¬ç”Ÿæˆ
            outputs = self.model.generate(
                input_ids=input_ids,  # è¾“å…¥çš„token ids
                attention_mask=attention_mask,  # è¾“å…¥çš„attention mask
                max_new_tokens=self.config.max_new_tokens,  # æœ€å¤§ç”Ÿæˆtokenæ•°
                temperature=self.config.temperature,  # ç”Ÿæˆçš„å¤šæ ·æ€§æ§åˆ¶å‚æ•°
                top_p=self.config.top_p,  # æ ¸é‡‡æ ·å‚æ•°
                do_sample=self.config.do_sample,  # æ˜¯å¦å¯ç”¨é‡‡æ ·
                pad_token_id=self.tokenizer.eos_token_id  # ä½¿ç”¨æ¨¡å‹çš„eos tokenä½œä¸ºå¡«å……token
            )

            # è§£ç ç”Ÿæˆçš„tokenä¸ºæ–‡æœ¬ï¼Œå¹¶æ¸…ç†æ‰ç‰¹æ®Štoken
            generated_text = self.tokenizer.decode(
                outputs[0][input_ids.shape[1]:],  # è·³è¿‡è¾“å…¥çš„éƒ¨åˆ†ï¼Œåªè§£ç ç”Ÿæˆçš„éƒ¨åˆ†
                skip_special_tokens=True,  # è·³è¿‡ç‰¹æ®Štoken
                clean_up_tokenization_spaces=True  # æ¸…ç†å¤šä½™çš„ç©ºæ ¼
            ).strip()  # å»æ‰ä¸¤ç«¯çš„ç©ºæ ¼

            logger.info("âœ… åŒæ­¥ç”Ÿæˆå®Œæˆ")
            logger.info(generated_text)
            return generated_text  # è¿”å›ç”Ÿæˆçš„æ–‡æœ¬

        except torch.cuda.OutOfMemoryError:
            logger.error("âš ï¸ CUDAå†…å­˜ä¸è¶³ï¼Œå°è¯•å‡å°max_new_tokens")  # å¦‚æœGPUå†…å­˜ä¸è¶³ï¼Œè®°å½•é”™è¯¯
            raise
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¤±è´¥: {str(e)}")  # å¦‚æœç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°å…¶ä»–é”™è¯¯ï¼Œè®°å½•æ—¥å¿—å¹¶æŠ›å‡ºå¼‚å¸¸
            raise
    def __enter__(self):
        """è¿›å…¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        return self  # è¿”å›å½“å‰å®ä¾‹

    def __exit__(self, exc_type, exc_val, exc_tb):
        """é€€å‡ºä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œæ¸…ç†èµ„æº"""
        if hasattr(self, "model"):
            del self.model  # åˆ é™¤æ¨¡å‹å®ä¾‹
        if torch.cuda.is_available():
            torch.cuda.empty_cache()  # æ¸…ç©ºCUDAç¼“å­˜
        logger.info("â™»ï¸ æ¨¡å‹èµ„æºå·²é‡Šæ”¾")  # è®°å½•èµ„æºé‡Šæ”¾æ—¥å¿—
