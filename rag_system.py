# å¯¼å…¥å¿…è¦çš„åº“å’Œæ¨¡å—
import json
import logging  # æ—¥å¿—è®°å½•æ¨¡å—
from pathlib import Path  # è·¯å¾„å¤„ç†åº“
from typing import Generator, Optional, List, Tuple, Dict, Any  # ç±»å‹æç¤ºæ”¯æŒ
import warnings  # è­¦å‘Šå¤„ç†
import torch  # PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
from langchain_community.vectorstores import FAISS  # FAISSå‘é‡æ•°æ®åº“é›†æˆ
from langchain_core.documents import Document  # æ–‡æ¡£å¯¹è±¡å®šä¹‰
from langchain_core.embeddings import Embeddings  # åµŒå…¥æ¨¡å‹æ¥å£
from langchain_ollama import OllamaLLM  # Ollamaè¯­è¨€æ¨¡å‹é›†æˆ
from rank_bm25 import BM25Okapi  # BM25æ£€ç´¢ç®—æ³•
from transformers import AutoModelForSequenceClassification, AutoTokenizer  # Transformeræ¨¡å‹
from config import Config  # è‡ªå®šä¹‰é…ç½®æ–‡ä»¶
from build_vector_store import VectorDBBuilder  # å‘é‡æ•°æ®åº“æ„å»ºå™¨
import numpy as np  # æ•°å€¼è®¡ç®—åº“
import jieba  # ä¸­æ–‡åˆ†è¯åº“

# ç¦ç”¨ä¸å¿…è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)

# é…ç½®æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)  # è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨


class RAGSystem:
    """RAGé—®ç­”ç³»ç»Ÿï¼Œæ”¯æŒæ–‡æ¡£æ£€ç´¢å’Œç”Ÿæˆå¼é—®ç­”

    ç‰¹æ€§ï¼š
    - è‡ªåŠ¨ç®¡ç†å‘é‡æ•°æ®åº“ç”Ÿå‘½å‘¨æœŸ
    - æ”¯æŒæµå¼ç”Ÿæˆå’ŒåŒæ­¥ç”Ÿæˆ
    - å¯é…ç½®çš„æ£€ç´¢ç­–ç•¥
    - å®Œå–„çš„é”™è¯¯å¤„ç†
    """

    def __init__(self, config: Config):
        """åˆå§‹åŒ–RAGç³»ç»Ÿ

        :param config: åŒ…å«æ‰€æœ‰é…ç½®å‚æ•°çš„Configå¯¹è±¡
        """
        self.config = config  # ä¿å­˜é…ç½®å¯¹è±¡
        self.vector_store: Optional[FAISS] = None  # FAISSå‘é‡æ•°æ®åº“å®ä¾‹
        self.llm: Optional[OllamaLLM] = None  # Ollamaè¯­è¨€æ¨¡å‹å®ä¾‹
        self.embeddings: Optional[Embeddings] = None  # åµŒå…¥æ¨¡å‹å®ä¾‹
        self.rerank_model = None  # é‡æ’åºæ¨¡å‹
        self.vector_db_build = VectorDBBuilder(config)  # å‘é‡æ•°æ®åº“æ„å»ºå™¨å®ä¾‹

        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self._init_logging()  # åˆå§‹åŒ–æ—¥å¿—é…ç½®
        self._init_embeddings()  # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self._init_vector_store()  # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self._init_bm25_retriever()  # åˆå§‹åŒ–BM25æ£€ç´¢å™¨
        self._init_llm()  # åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹
        self._init_rerank_model()  # åˆå§‹åŒ–é‡æ’åºæ¨¡å‹

    def _tokenize(self, text: str) -> List[str]:
        """ä¸“ä¸šä¸­æ–‡åˆ†è¯å¤„ç†
        :param text: å¾…åˆ†è¯çš„æ–‡æœ¬
        :return: åˆ†è¯åçš„è¯é¡¹åˆ—è¡¨
        """
        return [word for word in jieba.cut(text) if word.strip()]

    def _init_logging(self):
        """åˆå§‹åŒ–æ—¥å¿—é…ç½®"""
        logging.basicConfig(
            level=logging.INFO,  # æ—¥å¿—çº§åˆ«è®¾ä¸ºINFO
            format="%(asctime)s - %(levelname)s - %(message)s",  # æ—¥å¿—æ ¼å¼
            handlers=[logging.StreamHandler()]  # è¾“å‡ºåˆ°æ§åˆ¶å°
        )

    def _init_embeddings(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        try:
            logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
            # é€šè¿‡æ„å»ºå™¨åˆ›å»ºåµŒå…¥æ¨¡å‹å®ä¾‹
            self.embeddings = self.vector_db_build.create_embeddings()
            logger.info("âœ… åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error("âŒ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
            raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–åµŒå…¥æ¨¡å‹: {str(e)}")

    def _init_vector_store(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        try:
            vector_path = Path(self.config.vector_db_path)  # è·å–å‘é‡åº“è·¯å¾„

            # æ£€æŸ¥ç°æœ‰å‘é‡æ•°æ®åº“æ˜¯å¦å­˜åœ¨
            if vector_path.exists():
                logger.info("ğŸ” æ­£åœ¨åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“...")
                if not self.embeddings:
                    raise ValueError("åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–")

                # åŠ è½½æœ¬åœ°FAISSæ•°æ®åº“
                self.vector_store = FAISS.load_local(
                    folder_path=str(vector_path),
                    embeddings=self.embeddings,
                    allow_dangerous_deserialization=True  # å…è®¸åŠ è½½æ—§ç‰ˆæœ¬åºåˆ—åŒ–æ•°æ®
                )
                logger.info(f"âœ… å·²åŠ è½½å‘é‡æ•°æ®åº“ï¼š{vector_path}")
            else:
                # æ„å»ºæ–°å‘é‡æ•°æ®åº“
                logger.warning("âš ï¸ æœªæ‰¾åˆ°ç°æœ‰å‘é‡æ•°æ®åº“ï¼Œæ­£åœ¨æ„å»ºæ–°æ•°æ®åº“...")
                self.vector_store = self.vector_db_build.build_vector_store()
                logger.info(f"âœ… æ–°å»ºå‘é‡æ•°æ®åº“å·²ä¿å­˜è‡³ï¼š{vector_path}")
        except Exception as e:
            logger.error("âŒ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥")
            raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–å‘é‡æ•°æ®åº“: {str(e)}")

    def _init_rerank_model(self):
        """åˆå§‹åŒ–é‡æ’åºæ¨¡å‹"""
        try:
            logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–rerankæ¨¡å‹...")
            # ä»HuggingFaceåŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
            self.rerank_model = AutoModelForSequenceClassification.from_pretrained(
                self.config.rerank_model_path
            )
            self.rerank_tokenizer = AutoTokenizer.from_pretrained(self.config.rerank_model_path)
            logger.info("âœ… rerankæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ rerankæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–rerankæ¨¡å‹: {str(e)}")

    def _init_llm(self):
        """åˆå§‹åŒ–Ollamaå¤§è¯­è¨€æ¨¡å‹"""
        try:
            logger.info("ğŸš€ æ­£åœ¨åˆå§‹åŒ–Ollamaæ¨¡å‹...")
            # åˆ›å»ºOllamaLLMå®ä¾‹
            self.llm = OllamaLLM(
                model="deepseek-r1:1.5b",  # æ¨¡å‹åç§°
                base_url=self.config.ollama_base_url,  # OllamaæœåŠ¡åœ°å€
                temperature=self.config.llm_temperature,  # æ¸©åº¦å‚æ•°æ§åˆ¶éšæœºæ€§
                num_predict=self.config.llm_max_tokens,  # æœ€å¤§ç”Ÿæˆtokenæ•°
                system="ä½ æ˜¯ä¸€ä½åŒ–å·¥å®‰å…¨ä¸“å®¶ï¼Œè¯·ä¸“ä¸šä¸”å‡†ç¡®åœ°å›ç­”é—®é¢˜ã€‚",  # ç³»ç»Ÿæç¤ºè¯
                stop=["<|im_end|>"]  # åœæ­¢ç”Ÿæˆæ ‡è®°
            )

            # æµ‹è¯•æ¨¡å‹è¿æ¥
            test_prompt = "æµ‹è¯•è¿æ¥"
            self.llm.invoke(test_prompt)
            logger.info("âœ… Ollamaæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ Ollamaæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–Ollamaæ¨¡å‹: {str(e)}")

    def _init_bm25_retriever(self):
        """åˆå§‹åŒ–BM25æ£€ç´¢å™¨ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
        try:
            logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–BM25æ£€ç´¢å™¨...")

            # éªŒè¯å‘é‡åº“æ˜¯å¦åŒ…å«æ–‡æ¡£
            if not self.vector_store.docstore._dict:
                raise ValueError("å‘é‡åº“ä¸­æ— å¯ç”¨æ–‡æ¡£")

            # ä»å‘é‡åº“åŠ è½½æ‰€æœ‰æ–‡æ¡£å†…å®¹
            all_docs = self.vector_store.docstore._dict.values()
            self.bm25_docs = [doc.page_content for doc in all_docs]
            self.doc_metadata = [doc.metadata for doc in all_docs]

            # ä¸­æ–‡åˆ†è¯å¤„ç†
            tokenized_docs = [self._tokenize(doc) for doc in self.bm25_docs]

            # éªŒè¯åˆ†è¯ç»“æœæœ‰æ•ˆæ€§
            if len(tokenized_docs) == 0 or all(len(d) == 0 for d in tokenized_docs):
                raise ValueError("æ–‡æ¡£åˆ†è¯åä¸ºç©ºï¼Œè¯·æ£€æŸ¥åˆ†è¯é€»è¾‘")

            # åˆå§‹åŒ–BM25æ¨¡å‹
            self.bm25 = BM25Okapi(tokenized_docs)

            logger.info(f"âœ… BM25åˆå§‹åŒ–å®Œæˆï¼Œæ–‡æ¡£æ•°ï¼š{len(self.bm25_docs)}")
        except Exception as e:
            logger.error(f"âŒ BM25åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise RuntimeError(f"BM25åˆå§‹åŒ–å¤±è´¥: {str(e)}")

    def _hybrid_retrieve(self, question: str) -> List[Dict[str, Any]]:
        """æ··åˆæ£€ç´¢æµç¨‹ï¼ˆå‘é‡+BM25ï¼‰

        :param question: ç”¨æˆ·é—®é¢˜
        :return: åŒ…å«æ–‡æ¡£å’Œæ£€ç´¢ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
        """
        results = []

        # å‘é‡æ£€ç´¢éƒ¨åˆ†
        vector_results = self.vector_store.similarity_search_with_score(
            question, k=self.config.vector_top_k  # è·å–top kç»“æœ
        )
        for doc, score in vector_results:
            # å°†åˆ†æ•°è½¬æ¢ä¸ºæ ‡å‡†ä½™å¼¦å€¼ï¼ˆ0~1èŒƒå›´ï¼‰
            norm_score = (score + 1) / 2  # å¦‚æœåŸå§‹èŒƒå›´æ˜¯[-1,1]
            results.append({
                "doc": doc,
                "score": norm_score,  # ä½¿ç”¨å½’ä¸€åŒ–åçš„åˆ†æ•°
                "type": "vector",
                "source": doc.metadata.get("source", "unknown")
            })
            logger.info(f"ğŸ” å‘é‡æ£€ç´¢ç»“æœ: {doc.metadata['source']} - åˆ†æ•°: {score:.4f}")


        # BM25æ£€ç´¢éƒ¨åˆ†
        tokenized_query = self._tokenize(question)  # é—®é¢˜åˆ†è¯
        bm25_scores = self.bm25.get_scores(tokenized_query)  # è®¡ç®—BM25åˆ†æ•°
        # è·å–top kçš„ç´¢å¼•ï¼ˆå€’åºæ’åˆ—ï¼‰
        top_bm25_indices = np.argsort(bm25_scores)[-self.config.bm25_top_k:][::-1]

        for idx in top_bm25_indices:
            doc = Document(
                page_content=self.bm25_docs[idx],
                metadata=self.doc_metadata[idx]
            )
            results.append({
                "doc": doc,
                "score": float(bm25_scores[idx]),
                "type": "bm25",
                "source": doc.metadata.get("source", "unknown")
            })
            logger.info(f"ğŸ” BM25æ£€ç´¢ç»“æœ: {doc.metadata['source']} - åˆ†æ•°: {bm25_scores[idx]:.4f}")

        logger.info(f"ğŸ“š æ··åˆæ£€ç´¢åå¾—åˆ°{len(results)}ç¯‡æ–‡æ¡£")
        return results

    def _safe_normalize(self,scores: List[float]) -> List[float]:
        """å®‰å…¨å½’ä¸€åŒ–å¤„ç†"""
        if len(scores) == 0:
            return []

        min_val = min(scores)
        max_val = max(scores)

        # å¤„ç†å¸¸æ•°æƒ…å†µ
        if max_val == min_val:
            return [0.5] * len(scores)  # è¿”å›ä¸­æ€§å€¼

        return [(x - min_val) / (max_val - min_val) for x in scores]

    def _distribution_aware_normalize(self,scores: List[float], method: str) -> List[float]:
        """åˆ†å¸ƒæ„ŸçŸ¥çš„å½’ä¸€åŒ–"""
        if method == "robust":
            # ä½¿ç”¨å››åˆ†ä½æ•°é²æ£’å½’ä¸€åŒ–
            q25, q75 = np.percentile(scores, [25, 75])
            iqr = q75 - q25
            if iqr == 0:
                return [(x - q25) for x in scores]
            return [(x - q25) / iqr for x in scores]
        elif method == "zscore":
            # æ ‡å‡†Z-scoreå½’ä¸€åŒ–
            mean = np.mean(scores)
            std = np.std(scores) + 1e-9
            return [(x - mean) / std for x in scores]
        else:
            return self._safe_normalize(scores)

    def _normalize_scores(self, results: List[Dict]) -> List[Dict]:
        # æŒ‰ç±»å‹åˆ†ç»„
        vector_scores = [res["score"] for res in results if res["type"] == "vector"]
        bm25_scores = [res["score"] for res in results if res["type"] == "bm25"]

        # å·®å¼‚åŒ–å¤„ç†
        vector_norm = self._distribution_aware_normalize(vector_scores, method="zscore")
        bm25_norm = self._distribution_aware_normalize(bm25_scores, method="robust")

        # åˆå¹¶ç»“æœ
        idx_vec = 0
        idx_bm25 = 0
        for res in results:
            if res["type"] == "vector":
                res["norm_score"] = vector_norm[idx_vec]
                idx_vec += 1
            else:
                res["norm_score"] = bm25_norm[idx_bm25]
                idx_bm25 += 1
        for res in results:
            # Sigmoidå‹ç¼©åˆ°(0,1)åŒºé—´
            res["norm_score"] = 1 / (1 + np.exp(-res["norm_score"]))

        for res in results:
            logger.info(
                f"ğŸ“Š å½’ä¸€åŒ–åˆ†æ•°: {res['source']} - "
                f"åŸå§‹åˆ†æ•°: {res['score']:.4f} - "
                f"å½’ä¸€åŒ–åˆ†æ•°: {res['norm_score']:.4f}"
            )
        return results

    def _rerank_documents(self, results: List[Dict], question: str) -> List[Dict]:
        """ä½¿ç”¨é‡æ’åºæ¨¡å‹ä¼˜åŒ–æ£€ç´¢ç»“æœ

        :param results: æ£€ç´¢ç»“æœåˆ—è¡¨
        :param question: åŸå§‹é—®é¢˜
        :return: é‡æ’åºåçš„ç»“æœåˆ—è¡¨
        """
        try:
            # å‡†å¤‡æ¨¡å‹è¾“å…¥å¯¹ï¼ˆé—®é¢˜-æ–‡æ¡£ï¼‰
            pairs = [(question, res["doc"].page_content) for res in results]

            # å¯¹è¾“å…¥è¿›è¡Œtokenizeå’Œæ‰¹å¤„ç†
            inputs = self.rerank_tokenizer(
                pairs,
                padding=True,  # è‡ªåŠ¨å¡«å……
                truncation=True,  # è‡ªåŠ¨æˆªæ–­
                max_length=512,  # æœ€å¤§é•¿åº¦é™åˆ¶
                return_tensors="pt"  # è¿”å›PyTorchå¼ é‡
            )

            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                outputs = self.rerank_model(**inputs)
                # ä½¿ç”¨sigmoidè½¬æ¢åˆ†æ•°
                rerank_scores = torch.sigmoid(outputs.logits).squeeze().tolist()

            # åˆå¹¶åˆ†æ•°
            for res, rerank_score in zip(results, rerank_scores):
                # åŠ æƒå¹³å‡ç­–ç•¥
                final_score = (
                        self.config.retrieval_weight * res["norm_score"] +
                        self.config.rerank_weight * rerank_score
                )
                res.update({
                    "rerank_score": rerank_score,
                    "final_score": final_score
                })

            # æŒ‰æœ€ç»ˆåˆ†æ•°é™åºæ’åˆ—
            return sorted(results, key=lambda x: x["final_score"], reverse=True)
        except Exception as e:
            logger.error(f"é‡æ’åºå¤±è´¥: {str(e)}")
            return results  # å¤±è´¥æ—¶è¿”å›åŸå§‹æ’åº

    def _retrieve_documents(self, question: str) -> Tuple[List[Document], List[Dict]]:
        """å®Œæ•´æ£€ç´¢æµç¨‹

        :param question: ç”¨æˆ·é—®é¢˜
        :return: (æ–‡æ¡£åˆ—è¡¨, åˆ†æ•°ä¿¡æ¯åˆ—è¡¨)
        """
        try:
            # æ··åˆæ£€ç´¢
            raw_results = self._hybrid_retrieve(question)
            if not raw_results:
                return [], []

            # åˆ†æ•°å½’ä¸€åŒ–
            norm_results = self._normalize_scores(raw_results)

            # é‡æ’åº
            reranked = self._rerank_documents(norm_results, question)

            # æ ¹æ®é˜ˆå€¼è¿‡æ»¤ç»“æœ
            filtered = [
                res for res in reranked
                if res["final_score"] >= self.config.similarity_threshold
            ]
            final_results = sorted(
                filtered,
                key=lambda x: x["final_score"],
                reverse=True
            )[:self.config.final_top_k]
            # æå–æ–‡æ¡£å’Œåˆ†æ•°ä¿¡æ¯
            docs = [res["doc"] for res in final_results]
            score_info = [{
                "source": res["source"],
                "type": res["type"],
                "vector_score": res.get("score", 0),  # å…¼å®¹ä¸åŒæ£€ç´¢ç±»å‹
                "bm25_score": res.get("score", 0),
                "rerank_score": res["rerank_score"],
                "final_score": res["final_score"]
            } for res in final_results]

            return docs, score_info
        except Exception as e:
            logger.error(f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {str(e)}")
            raise

    def _build_prompt(self, question: str, context: str) -> str:
        """æ„å»ºç¬¦åˆdeepseek-r1æ ¼å¼çš„æç¤ºæ¨¡æ¿

        :param question: ç”¨æˆ·é—®é¢˜
        :param context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
        :return: æ ¼å¼åŒ–åçš„æç¤ºå­—ç¬¦ä¸²
        """
        if context:
            return (
                "<|im_start|>system\n"
                "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„åŒ–å·¥å®‰å…¨é¢†åŸŸä¸“å®¶...\n"
                "ä¸Šä¸‹æ–‡ï¼š\n{context}\n"
                "<|im_end|>\n"
                "<|im_start|>user\n"
                "{question}\n"
                "<|im_end|>\n"
                "<|im_start|>assistant\n"
            ).format(question=question, context=context[:self.config.max_context_length])
        else:
            return (
                "<|im_start|>user\n"
                "{question}\n"
                "<|im_end|>\n"
                "<|im_start|>assistant\n"
            ).format(question=question)

    def _format_references(self, docs: List[Document], score_info: List[Dict]) -> List[Dict]:
        """æ ¼å¼åŒ–å‚è€ƒæ–‡æ¡£ä¿¡æ¯"""
        return [
            {
                "file": str(Path(info["source"]).name),  # æ–‡ä»¶å
                "content": doc.page_content,  # æˆªå–å‰500å­—ç¬¦
                "score": info["final_score"],  # ç»¼åˆè¯„åˆ†
                "type": info["type"],  # æ£€ç´¢ç±»å‹
                "full_path": info["source"]  # å®Œæ•´æ–‡ä»¶è·¯å¾„
            }
            for doc, info in zip(docs, score_info)
        ]
    def stream_query_model(self, question: str) -> Generator[str, None, None]:
        """çº¯æ¨¡å‹æµå¼ç”Ÿæˆï¼ˆä¸ç»è¿‡RAGï¼‰"""
        logger.info(f"ğŸŒ€ æ­£åœ¨ç›´æ¥æµå¼ç”Ÿæˆ: {question[:50]}...")
        try:
            if not question.strip():
                yield "âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜"
                return

            # æ„å»ºåŸºç¡€æç¤ºæ¨¡æ¿ï¼ˆä¸åŒ…å«ä¸Šä¸‹æ–‡ï¼‰
            prompt = (
                "<|im_start|>system\n"
                "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„åŒ–å·¥å®‰å…¨é¢†åŸŸä¸“å®¶ï¼Œè¯·ä¸“ä¸šä¸”å‡†ç¡®åœ°å›ç­”é—®é¢˜ã€‚\n"
                "<|im_end|>\n"
                "<|im_start|>user\n"
                f"{question}\n"
                "<|im_end|>\n"
                "<|im_start|>assistant\n"
            )

            try:
                full_response = ""
                for chunk in self.llm.stream(prompt):
                    cleaned_chunk = chunk.replace("<|im_end|>", "")
                    if cleaned_chunk:
                        # å‘é€ç”Ÿæˆå†…å®¹ï¼ˆä½œä¸ºæ™®é€šæ–‡æœ¬ï¼‰
                        yield json.dumps({
                            "type": "content",
                            "data": cleaned_chunk
                        }) + "\n"

            except Exception as e:
                logger.error(f"ç›´æ¥ç”Ÿæˆä¸­æ–­: {str(e)}")
                yield "\nâš ï¸ ç”Ÿæˆè¿‡ç¨‹å‘ç”Ÿæ„å¤–ä¸­æ–­ï¼Œè¯·ç¨åé‡è¯•"

        except Exception as e:
            logger.exception("ç›´æ¥æµå¼ç”Ÿæˆé”™è¯¯")
            yield "âš ï¸ ç³»ç»Ÿå¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯ï¼Œè¯·è”ç³»ç®¡ç†å‘˜"

    def stream_query_rag_with_kb(self, question: str) -> Generator[str, None, None]:
        """ç»“åˆçŸ¥è¯†åº“çš„æµå¼RAGç”Ÿæˆ"""
        logger.info(f"ğŸŒŠ æ­£åœ¨æµå¼å¤„ç†æŸ¥è¯¢: {question[:50]}...")
        if not question.strip():
            yield "âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜"
            return

        try:
            # é˜¶æ®µ1ï¼šæ–‡æ¡£æ£€ç´¢
            try:
                docs, score_info = self._retrieve_documents(question)
                if not docs:
                    yield "âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£..."
                    return
            except Exception as e:
                logger.error(f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {str(e)}")
                yield "âš ï¸ æ–‡æ¡£æ£€ç´¢æœåŠ¡æš‚æ—¶ä¸å¯ç”¨"
                return

            # æ ¼å¼åŒ–å‚è€ƒæ–‡æ¡£ä¿¡æ¯
            references = self._format_references(docs, score_info)

            # å‘é€å‚è€ƒæ–‡æ¡£ä¿¡æ¯ï¼ˆä½œä¸ºJSONï¼‰
            yield json.dumps({
                "type": "references",
                "data": references
            }) + "\n"  # æ·»åŠ æ¢è¡Œç¬¦ä½œä¸ºç»“æŸæ ‡è®°

            # é˜¶æ®µ2ï¼šæ„å»ºä¸Šä¸‹æ–‡
            context = "\n\n".join([
                f"ã€å‚è€ƒæ–‡æ¡£{i + 1}ã€‘{doc.page_content}\n"
                f"- æ¥æº: {Path(info['source']).name}\n"
                f"- ç»¼åˆç½®ä¿¡åº¦: {info['final_score'] * 100:.1f}%"
                for i, (doc, info) in enumerate(zip(docs, score_info))
            ])

            # é˜¶æ®µ3ï¼šæ„å»ºæç¤ºæ¨¡æ¿
            prompt = self._build_prompt(question, context)

            # é˜¶æ®µ4ï¼šæµå¼ç”Ÿæˆ
            try:
                full_response = ""
                for chunk in self.llm.stream(prompt):
                    cleaned_chunk = chunk.replace("<|im_end|>", "")
                    if cleaned_chunk:
                        # å‘é€ç”Ÿæˆå†…å®¹ï¼ˆä½œä¸ºæ™®é€šæ–‡æœ¬ï¼‰
                        yield json.dumps({
                            "type": "content",
                            "data": cleaned_chunk
                        }) + "\n"

            except Exception as e:
                logger.error(f"æµå¼ç”Ÿæˆä¸­æ–­: {str(e)}")
                yield json.dumps({
                    "type": "error",
                    "data": "\nâš ï¸ ç”Ÿæˆè¿‡ç¨‹å‘ç”Ÿæ„å¤–ä¸­æ–­"
                }) + "\n"

        except Exception as e:
            logger.exception("æµå¼å¤„ç†ä¸¥é‡é”™è¯¯")
            yield json.dumps({
                "type": "error",
                "data": "âš ï¸ ç³»ç»Ÿå¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯"
            }) + "\n"

