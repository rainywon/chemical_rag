# å¯¼å…¥å¿…è¦çš„åº“å’Œæ¨¡å—
import json
import logging  # æ—¥å¿—è®°å½•æ¨¡å—
from pathlib import Path  # è·¯å¾„å¤„ç†åº“
from typing import Generator, Optional, List, Tuple, Dict, Any  # ç±»å‹æç¤ºæ”¯æŒ
import warnings  # è­¦å‘Šå¤„ç†
import torch  # PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
from langchain_community.vectorstores import FAISS  # FAISSå‘é‡æ•°æ®åº“é›†æˆ
from langchain_core.documents import Document  # æ–‡æ¡£å¯¹è±¡å®šä¹‰
from langchain_core.embeddings import Embeddings  # åµŒå…¥æ¨¡å‹æ¥å£
from langchain_ollama import OllamaLLM  # Ollamaè¯­è¨€æ¨¡å‹é›†æˆ
from rank_bm25 import BM25Okapi  # BM25æ£€ç´¢ç®—æ³•
from transformers import AutoModelForSequenceClassification, AutoTokenizer  # Transformeræ¨¡å‹
from config import Config  # è‡ªå®šä¹‰é…ç½®æ–‡ä»¶
from build_vector_store import VectorDBBuilder  # å‘é‡æ•°æ®åº“æ„å»ºå™¨
import numpy as np  # æ•°å€¼è®¡ç®—åº“
import jieba  # ä¸­æ–‡åˆ†è¯åº“

# ç¦ç”¨ä¸å¿…è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)

# é…ç½®æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)  # è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨


class RAGSystem:
    """RAGé—®ç­”ç³»ç»Ÿï¼Œæ”¯æŒæ–‡æ¡£æ£€ç´¢å’Œç”Ÿæˆå¼é—®ç­”

    ç‰¹æ€§ï¼š
    - è‡ªåŠ¨ç®¡ç†å‘é‡æ•°æ®åº“ç”Ÿå‘½å‘¨æœŸ
    - æ”¯æŒæµå¼ç”Ÿæˆå’ŒåŒæ­¥ç”Ÿæˆ
    - å¯é…ç½®çš„æ£€ç´¢ç­–ç•¥
    - å®Œå–„çš„é”™è¯¯å¤„ç†
    """

    def __init__(self, config: Config):
        """åˆå§‹åŒ–RAGç³»ç»Ÿ

        :param config: åŒ…å«æ‰€æœ‰é…ç½®å‚æ•°çš„Configå¯¹è±¡
        """
        self.config = config  # ä¿å­˜é…ç½®å¯¹è±¡
        self.vector_store: Optional[FAISS] = None  # FAISSå‘é‡æ•°æ®åº“å®ä¾‹
        self.llm: Optional[OllamaLLM] = None  # Ollamaè¯­è¨€æ¨¡å‹å®ä¾‹
        self.embeddings: Optional[Embeddings] = None  # åµŒå…¥æ¨¡å‹å®ä¾‹
        self.rerank_model = None  # é‡æ’åºæ¨¡å‹
        self.vector_db_build = VectorDBBuilder(config)  # å‘é‡æ•°æ®åº“æ„å»ºå™¨å®ä¾‹

        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self._init_logging()  # åˆå§‹åŒ–æ—¥å¿—é…ç½®
        self._init_embeddings()  # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self._init_vector_store()  # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self._init_bm25_retriever()  # åˆå§‹åŒ–BM25æ£€ç´¢å™¨
        self._init_llm()  # åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹
        self._init_rerank_model()  # åˆå§‹åŒ–é‡æ’åºæ¨¡å‹

    def _tokenize(self, text: str) -> List[str]:
        """ä¸“ä¸šä¸­æ–‡åˆ†è¯å¤„ç†
        :param text: å¾…åˆ†è¯çš„æ–‡æœ¬
        :return: åˆ†è¯åçš„è¯é¡¹åˆ—è¡¨
        """
        return [word for word in jieba.cut(text) if word.strip()]

    def _init_logging(self):
        """åˆå§‹åŒ–æ—¥å¿—é…ç½®"""
        logging.basicConfig(
            level=logging.INFO,  # æ—¥å¿—çº§åˆ«è®¾ä¸ºINFO
            format="%(asctime)s - %(levelname)s - %(message)s",  # æ—¥å¿—æ ¼å¼
            handlers=[logging.StreamHandler()]  # è¾“å‡ºåˆ°æ§åˆ¶å°
        )

    def _init_embeddings(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        try:
            logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
            # é€šè¿‡æ„å»ºå™¨åˆ›å»ºåµŒå…¥æ¨¡å‹å®ä¾‹
            self.embeddings = self.vector_db_build.create_embeddings()
            logger.info("âœ… åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error("âŒ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
            raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–åµŒå…¥æ¨¡å‹: {str(e)}")

    def _init_vector_store(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        try:
            vector_path = Path(self.config.vector_db_path)  # è·å–å‘é‡åº“è·¯å¾„

            # æ£€æŸ¥ç°æœ‰å‘é‡æ•°æ®åº“æ˜¯å¦å­˜åœ¨
            if vector_path.exists():
                logger.info("ğŸ” æ­£åœ¨åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“...")
                if not self.embeddings:
                    raise ValueError("åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–")

                # åŠ è½½æœ¬åœ°FAISSæ•°æ®åº“
                self.vector_store = FAISS.load_local(
                    folder_path=str(vector_path),
                    embeddings=self.embeddings,
                    allow_dangerous_deserialization=True  # å…è®¸åŠ è½½æ—§ç‰ˆæœ¬åºåˆ—åŒ–æ•°æ®
                )
                logger.info(f"âœ… å·²åŠ è½½å‘é‡æ•°æ®åº“ï¼š{vector_path}")
            else:
                # æ„å»ºæ–°å‘é‡æ•°æ®åº“
                logger.warning("âš ï¸ æœªæ‰¾åˆ°ç°æœ‰å‘é‡æ•°æ®åº“ï¼Œæ­£åœ¨æ„å»ºæ–°æ•°æ®åº“...")
                self.vector_store = self.vector_db_build.build_vector_store()
                logger.info(f"âœ… æ–°å»ºå‘é‡æ•°æ®åº“å·²ä¿å­˜è‡³ï¼š{vector_path}")
        except Exception as e:
            logger.error("âŒ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥")
            raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–å‘é‡æ•°æ®åº“: {str(e)}")

    def _init_rerank_model(self):
        """åˆå§‹åŒ–é‡æ’åºæ¨¡å‹"""
        try:
            logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–rerankæ¨¡å‹...")
            # ä»HuggingFaceåŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
            self.rerank_model = AutoModelForSequenceClassification.from_pretrained(
                self.config.rerank_model_path
            )
            self.rerank_tokenizer = AutoTokenizer.from_pretrained(self.config.rerank_model_path)
            logger.info("âœ… rerankæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ rerankæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–rerankæ¨¡å‹: {str(e)}")

    def _init_llm(self):
        """åˆå§‹åŒ–Ollamaå¤§è¯­è¨€æ¨¡å‹"""
        try:
            logger.info("ğŸš€ æ­£åœ¨åˆå§‹åŒ–Ollamaæ¨¡å‹...")
            # åˆ›å»ºOllamaLLMå®ä¾‹
            self.llm = OllamaLLM(
                model="deepseek-r1:1.5b",  # æ¨¡å‹åç§°
                base_url=self.config.ollama_base_url,  # OllamaæœåŠ¡åœ°å€
                temperature=self.config.llm_temperature,  # æ¸©åº¦å‚æ•°æ§åˆ¶éšæœºæ€§
                num_predict=self.config.llm_max_tokens,  # æœ€å¤§ç”Ÿæˆtokenæ•°
                system="ä½ æ˜¯ä¸€ä½åŒ–å·¥å®‰å…¨ä¸“å®¶ï¼Œè¯·ä¸“ä¸šä¸”å‡†ç¡®åœ°å›ç­”é—®é¢˜ã€‚",  # ç³»ç»Ÿæç¤ºè¯
                stop=["<|im_end|>"]  # åœæ­¢ç”Ÿæˆæ ‡è®°
            )

            # æµ‹è¯•æ¨¡å‹è¿æ¥
            test_prompt = "æµ‹è¯•è¿æ¥"
            self.llm.invoke(test_prompt)
            logger.info("âœ… Ollamaæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ Ollamaæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–Ollamaæ¨¡å‹: {str(e)}")

    def _init_bm25_retriever(self):
        """åˆå§‹åŒ–BM25æ£€ç´¢å™¨ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
        try:
            logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–BM25æ£€ç´¢å™¨...")

            # éªŒè¯å‘é‡åº“æ˜¯å¦åŒ…å«æ–‡æ¡£
            if not self.vector_store.docstore._dict:
                raise ValueError("å‘é‡åº“ä¸­æ— å¯ç”¨æ–‡æ¡£")

            # ä»å‘é‡åº“åŠ è½½æ‰€æœ‰æ–‡æ¡£å†…å®¹
            all_docs = self.vector_store.docstore._dict.values()
            self.bm25_docs = [doc.page_content for doc in all_docs]
            self.doc_metadata = [doc.metadata for doc in all_docs]

            # ä¸­æ–‡åˆ†è¯å¤„ç†
            tokenized_docs = [self._tokenize(doc) for doc in self.bm25_docs]

            # éªŒè¯åˆ†è¯ç»“æœæœ‰æ•ˆæ€§
            if len(tokenized_docs) == 0 or all(len(d) == 0 for d in tokenized_docs):
                raise ValueError("æ–‡æ¡£åˆ†è¯åä¸ºç©ºï¼Œè¯·æ£€æŸ¥åˆ†è¯é€»è¾‘")

            # åˆå§‹åŒ–BM25æ¨¡å‹
            self.bm25 = BM25Okapi(tokenized_docs)

            logger.info(f"âœ… BM25åˆå§‹åŒ–å®Œæˆï¼Œæ–‡æ¡£æ•°ï¼š{len(self.bm25_docs)}")
        except Exception as e:
            logger.error(f"âŒ BM25åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise RuntimeError(f"BM25åˆå§‹åŒ–å¤±è´¥: {str(e)}")

    def _enhance_query(self, original_query: str) -> List[str]:
        """æŸ¥è¯¢å¢å¼ºä¸æ‰©å±•
        
        :param original_query: åŸå§‹æŸ¥è¯¢
        :return: å¢å¼ºåçš„æŸ¥è¯¢åˆ—è¡¨
        """
        # åŸºç¡€æŸ¥è¯¢å§‹ç»ˆåŒ…å«åŸå§‹æŸ¥è¯¢
        queries = [original_query]
        
        try:
            # 1. ç§»é™¤åœç”¨è¯çš„ç®€åŒ–æŸ¥è¯¢
            stop_words = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 
                         'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 
                         'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'}
            
            words = self._tokenize(original_query)
            simplified_query = ' '.join([w for w in words if w not in stop_words])
            
            if simplified_query and simplified_query != original_query:
                queries.append(simplified_query)
            
            # 2. ä¸“ä¸šæœ¯è¯­æå–å’Œé‡ç‚¹å…³æ³¨
            # åŒ–å·¥å®‰å…¨é¢†åŸŸçš„ä¸“ä¸šæœ¯è¯­åŠå…¶æƒé‡
            chemical_terms = {
                'åŒ–å­¦å“': 2.0, 'æ˜“ç‡ƒ': 2.0, 'æ˜“çˆ†': 2.0, 'æœ‰æ¯’': 2.0, 'è…èš€': 2.0, 
                'å±é™©': 1.5, 'å®‰å…¨': 1.5, 'é˜²æŠ¤': 1.5, 'äº‹æ•…': 1.5, 'æ³„æ¼': 2.0,
                'çˆ†ç‚¸': 2.0, 'ç«ç¾': 2.0, 'ä¸­æ¯’': 2.0, 'åº”æ€¥': 1.5, 'å¤„ç½®': 1.5,
                'é£é™©': 1.5, 'å±å®³': 1.5, 'é˜²èŒƒ': 1.5, 'æªæ–½': 1.0, 'æ“ä½œ': 1.0,
                'ååº”': 1.8, 'ç‰©è´¨': 1.8, 'æ°”ä½“': 1.8, 'æ¶²ä½“': 1.8, 'å›ºä½“': 1.8,
                'æµ“åº¦': 1.8, 'æ¸©åº¦': 1.8, 'å‹åŠ›': 1.8, 'å‚¨å­˜': 1.8, 'è¿è¾“': 1.8
            }
            
            # æå–æŸ¥è¯¢ä¸­çš„ä¸“ä¸šæœ¯è¯­
            matched_terms = []
            term_weights = {}
            
            for term, weight in chemical_terms.items():
                if term in original_query:
                    matched_terms.append(term)
                    term_weights[term] = weight
            
            if matched_terms:
                # æ„å»ºä¸“ä¸šæœ¯è¯­å¢å¼ºçš„æŸ¥è¯¢
                terms_query = ' '.join(matched_terms)
                if terms_query != original_query and len(matched_terms) >= 2:
                    queries.append(terms_query)
                
                # æ„å»ºåŠ æƒæŸ¥è¯¢ï¼Œå¤åˆ¶é‡è¦æœ¯è¯­
                weighted_query_parts = []
                for word in words:
                    if word in term_weights:
                        # æ ¹æ®æƒé‡é‡å¤æœ¯è¯­
                        repeat = max(1, int(term_weights[word]))
                        weighted_query_parts.extend([word] * repeat)
                    else:
                        weighted_query_parts.append(word)
                
                weighted_query = ' '.join(weighted_query_parts)
                if weighted_query != original_query:
                    queries.append(weighted_query)
            
            logger.info(f"ğŸ“ æŸ¥è¯¢å¢å¼º: ä»åŸå§‹æŸ¥è¯¢'{original_query}'ç”Ÿæˆäº†{len(queries)}ä¸ªå˜ä½“")
            return queries
            
        except Exception as e:
            logger.warning(f"âš ï¸ æŸ¥è¯¢å¢å¼ºå¤±è´¥: {str(e)}")
            return [original_query]  # è¿”å›åŸå§‹æŸ¥è¯¢

    def _hybrid_retrieve(self, question: str) -> List[Dict[str, Any]]:
        """æ··åˆæ£€ç´¢æµç¨‹ï¼ˆå‘é‡+BM25ï¼‰

        :param question: ç”¨æˆ·é—®é¢˜
        :return: åŒ…å«æ–‡æ¡£å’Œæ£€ç´¢ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
        """
        results = []
        
        # æŸ¥è¯¢å¢å¼ºå¤„ç†
        enhanced_queries = self._enhance_query(question)
        
        # åŠ¨æ€ç¡®å®šæ£€ç´¢ç­–ç•¥æƒé‡
        vector_weight, bm25_weight = self._determine_retrieval_weights(question)
        logger.info(f"ğŸ”€ åŠ¨æ€æƒé‡: å‘é‡æ£€ç´¢={vector_weight:.2f}, BM25æ£€ç´¢={bm25_weight:.2f}")

        # å‘é‡æ£€ç´¢éƒ¨åˆ†
        all_vector_results = []
        for query in enhanced_queries:
            vector_results = self.vector_store.similarity_search_with_score(
                query, k=self.config.vector_top_k  # è·å–top kç»“æœ
            )
            all_vector_results.extend(vector_results)
        
        # å»é‡å¹¶ä¿ç•™æœ€é«˜åˆ†æ•°
        unique_vector_results = {}
        for doc, score in all_vector_results:
            doc_id = doc.metadata.get("source", "") + str(hash(doc.page_content))
            norm_score = (score + 1) / 2  # è½¬æ¢ä¸ºæ ‡å‡†ä½™å¼¦å€¼ï¼ˆ0~1èŒƒå›´ï¼‰
            
            # å¦‚æœæ–‡æ¡£å·²å­˜åœ¨ä¸”æ–°åˆ†æ•°æ›´é«˜ï¼Œåˆ™æ›´æ–°
            if doc_id not in unique_vector_results or norm_score > unique_vector_results[doc_id][1]:
                unique_vector_results[doc_id] = (doc, norm_score)
        
        # å°†å»é‡åçš„ç»“æœæ·»åŠ åˆ°ç»“æœåˆ—è¡¨
        for doc, score in unique_vector_results.values():
            results.append({
                "doc": doc,
                "score": score * vector_weight,  # åº”ç”¨åŠ¨æ€æƒé‡
                "raw_score": score,
                "type": "vector",
                "source": doc.metadata.get("source", "unknown")
            })
            logger.info(f"ğŸ” å‘é‡æ£€ç´¢ç»“æœ: {doc.metadata['source']} - åˆ†æ•°: {score:.4f}")

        # BM25æ£€ç´¢éƒ¨åˆ†
        all_bm25_scores = {}
        for query in enhanced_queries:
            tokenized_query = self._tokenize(query)  # é—®é¢˜åˆ†è¯
            bm25_scores = self.bm25.get_scores(tokenized_query)  # è®¡ç®—BM25åˆ†æ•°
            
            # æ›´æ–°æœ€é«˜åˆ†æ•°
            for idx, score in enumerate(bm25_scores):
                if idx not in all_bm25_scores or score > all_bm25_scores[idx]:
                    all_bm25_scores[idx] = score
        
        # è·å–top kçš„ç´¢å¼•ï¼ˆå€’åºæ’åˆ—ï¼‰
        top_bm25_indices = np.argsort(list(all_bm25_scores.values()))[-self.config.bm25_top_k:][::-1]
        top_bm25_indices = [list(all_bm25_scores.keys())[i] for i in top_bm25_indices]

        for idx in top_bm25_indices:
            doc = Document(
                page_content=self.bm25_docs[idx],
                metadata=self.doc_metadata[idx]
            )
            
            bm25_score = float(all_bm25_scores[idx])
            results.append({
                "doc": doc,
                "score": bm25_score * bm25_weight,  # åº”ç”¨åŠ¨æ€æƒé‡
                "raw_score": bm25_score,
                "type": "bm25",
                "source": doc.metadata.get("source", "unknown")
            })
            logger.info(f"ğŸ” BM25æ£€ç´¢ç»“æœ: {doc.metadata['source']} - åˆ†æ•°: {bm25_score:.4f}")

        logger.info(f"ğŸ“š æ··åˆæ£€ç´¢åå¾—åˆ°{len(results)}ç¯‡æ–‡æ¡£")
        return results
    
    def _determine_retrieval_weights(self, question: str) -> Tuple[float, float]:
        """åŠ¨æ€ç¡®å®šæ£€ç´¢ç­–ç•¥æƒé‡
        
        :param question: ç”¨æˆ·é—®é¢˜
        :return: (å‘é‡æ£€ç´¢æƒé‡, BM25æ£€ç´¢æƒé‡)
        """
        # é»˜è®¤æƒé‡
        default_vector = 0.5
        default_bm25 = 0.5
        
        try:
            # 1. æ£€æµ‹é—®é¢˜ç±»å‹ç‰¹å¾
            
            # äº‹å®å‹é—®é¢˜ç‰¹å¾è¯ï¼ˆåå‘BM25ï¼‰
            factual_indicators = ['ä»€ä¹ˆæ˜¯', 'å®šä¹‰', 'å¦‚ä½•', 'æ€ä¹ˆ', 'å“ªäº›', 
                               'è°', 'ä½•æ—¶', 'ä¸ºä»€ä¹ˆ', 'å¤šå°‘', 'æ•°æ®',
                               'æ ‡å‡†æ˜¯', 'è¦æ±‚æ˜¯']
            
            # æ¦‚å¿µå‹é—®é¢˜ç‰¹å¾è¯ï¼ˆåå‘å‘é‡æ£€ç´¢ï¼‰
            conceptual_indicators = ['è§£é‡Š', 'åˆ†æ', 'è¯„ä»·', 'æ¯”è¾ƒ', 'åŒºåˆ«',
                                  'å…³ç³»', 'å½±å“', 'åŸç†', 'æœºåˆ¶', 'æ€è€ƒ',
                                  'å¯èƒ½', 'å»ºè®®', 'é¢„æµ‹', 'æ¨æµ‹']
                               
            # è®¡ç®—å„ç±»ç‰¹å¾å‡ºç°æ¬¡æ•°
            factual_count = sum(1 for term in factual_indicators if term in question)
            conceptual_count = sum(1 for term in conceptual_indicators if term in question)
            
            # 2. è€ƒè™‘é—®é¢˜é•¿åº¦å› ç´ 
            # è¾ƒçŸ­é—®é¢˜é€šå¸¸æ˜¯ç›´æ¥æŸ¥è¯¢ï¼Œé€‚åˆå…³é”®è¯åŒ¹é…
            # è¾ƒé•¿é—®é¢˜å¯èƒ½æ˜¯å¤æ‚æ¦‚å¿µï¼Œé€‚åˆè¯­ä¹‰åŒ¹é…
            query_length = len(question)
            length_factor = min(1.0, query_length / 50)  # æ ‡å‡†åŒ–é•¿åº¦å› ç´ 
            
            # 3. ç¡®å®šæœ€ç»ˆæƒé‡
            if factual_count > conceptual_count:
                # äº‹å®å‹é—®é¢˜ï¼šå¢åŠ BM25æƒé‡
                bm25_weight = 0.6 + 0.1 * min(factual_count, 3)
                vector_weight = 1.0 - bm25_weight
            elif conceptual_count > factual_count:
                # æ¦‚å¿µå‹é—®é¢˜ï¼šå¢åŠ å‘é‡æƒé‡
                vector_weight = 0.6 + 0.1 * min(conceptual_count, 3) + 0.1 * length_factor
                bm25_weight = 1.0 - vector_weight
            else:
                # æ··åˆç±»å‹é—®é¢˜ï¼šæ ¹æ®é•¿åº¦å¾®è°ƒ
                vector_weight = default_vector + 0.1 * length_factor
                bm25_weight = 1.0 - vector_weight
                
            # ç¡®ä¿æƒé‡ç›¸åŠ ä¸º1
            total = vector_weight + bm25_weight
            return vector_weight/total, bm25_weight/total
            
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ¨æ€æƒé‡è®¡ç®—å¤±è´¥: {str(e)}")
            return default_vector, default_bm25

    def _safe_normalize(self,scores: List[float]) -> List[float]:
        """å®‰å…¨å½’ä¸€åŒ–å¤„ç†"""
        if len(scores) == 0:
            return []

        min_val = min(scores)
        max_val = max(scores)

        # å¤„ç†å¸¸æ•°æƒ…å†µ
        if max_val == min_val:
            return [0.5] * len(scores)  # è¿”å›ä¸­æ€§å€¼

        return [(x - min_val) / (max_val - min_val) for x in scores]

    def _distribution_aware_normalize(self,scores: List[float], method: str) -> List[float]:
        """åˆ†å¸ƒæ„ŸçŸ¥çš„å½’ä¸€åŒ–"""
        if method == "robust":
            # ä½¿ç”¨å››åˆ†ä½æ•°é²æ£’å½’ä¸€åŒ–
            q25, q75 = np.percentile(scores, [25, 75])
            iqr = q75 - q25
            if iqr == 0:
                return [(x - q25) for x in scores]
            return [(x - q25) / iqr for x in scores]
        elif method == "zscore":
            # æ ‡å‡†Z-scoreå½’ä¸€åŒ–
            mean = np.mean(scores)
            std = np.std(scores) + 1e-9
            return [(x - mean) / std for x in scores]
        else:
            return self._safe_normalize(scores)

    def _normalize_scores(self, results: List[Dict]) -> List[Dict]:
        # æŒ‰ç±»å‹åˆ†ç»„
        vector_scores = [res["score"] for res in results if res["type"] == "vector"]
        bm25_scores = [res["score"] for res in results if res["type"] == "bm25"]

        # å·®å¼‚åŒ–å¤„ç†
        vector_norm = self._distribution_aware_normalize(vector_scores, method="zscore")
        bm25_norm = self._distribution_aware_normalize(bm25_scores, method="robust")

        # åˆå¹¶ç»“æœ
        idx_vec = 0
        idx_bm25 = 0
        for res in results:
            if res["type"] == "vector":
                res["norm_score"] = vector_norm[idx_vec]
                idx_vec += 1
            else:
                res["norm_score"] = bm25_norm[idx_bm25]
                idx_bm25 += 1
        for res in results:
            # Sigmoidå‹ç¼©åˆ°(0,1)åŒºé—´
            res["norm_score"] = 1 / (1 + np.exp(-res["norm_score"]))

        for res in results:
            logger.info(
                f"ğŸ“Š å½’ä¸€åŒ–åˆ†æ•°: {res['source']} - "
                f"åŸå§‹åˆ†æ•°: {res['score']:.4f} - "
                f"å½’ä¸€åŒ–åˆ†æ•°: {res['norm_score']:.4f}"
            )
        return results

    def _rerank_documents(self, results: List[Dict], question: str) -> List[Dict]:
        """ä½¿ç”¨é‡æ’åºæ¨¡å‹ä¼˜åŒ–æ£€ç´¢ç»“æœ

        :param results: æ£€ç´¢ç»“æœåˆ—è¡¨
        :param question: åŸå§‹é—®é¢˜
        :return: é‡æ’åºåçš„ç»“æœåˆ—è¡¨
        """
        try:
            # å‡†å¤‡æ¨¡å‹è¾“å…¥å¯¹ï¼ˆé—®é¢˜-æ–‡æ¡£ï¼‰
            pairs = [(question, res["doc"].page_content) for res in results]

            # å¯¹è¾“å…¥è¿›è¡Œtokenizeå’Œæ‰¹å¤„ç†
            inputs = self.rerank_tokenizer(
                pairs,
                padding=True,  # è‡ªåŠ¨å¡«å……
                truncation=True,  # è‡ªåŠ¨æˆªæ–­
                max_length=512,  # æœ€å¤§é•¿åº¦é™åˆ¶
                return_tensors="pt"  # è¿”å›PyTorchå¼ é‡
            )

            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                outputs = self.rerank_model(**inputs)
                # ä½¿ç”¨sigmoidè½¬æ¢åˆ†æ•°
                rerank_scores = torch.sigmoid(outputs.logits).squeeze().tolist()
                
                # ç¡®ä¿rerank_scoresæ˜¯åˆ—è¡¨
                if not isinstance(rerank_scores, list):
                    rerank_scores = [rerank_scores]

            # åˆå¹¶åˆ†æ•°
            for res, rerank_score in zip(results, rerank_scores):
                # åŠ æƒå¹³å‡ç­–ç•¥
                final_score = (
                        self.config.retrieval_weight * res["norm_score"] +
                        self.config.rerank_weight * rerank_score
                )
                res.update({
                    "rerank_score": rerank_score,
                    "final_score": final_score
                })

            # æŒ‰æœ€ç»ˆåˆ†æ•°é™åºæ’åˆ—
            sorted_results = sorted(results, key=lambda x: x["final_score"], reverse=True)
            
            # åº”ç”¨å¤šæ ·æ€§å¢å¼ºç­–ç•¥
            return self._diversify_results(sorted_results)
            
        except Exception as e:
            logger.error(f"é‡æ’åºå¤±è´¥: {str(e)}")
            return results  # å¤±è´¥æ—¶è¿”å›åŸå§‹æ’åº
    
    def _diversify_results(self, ranked_results: List[Dict]) -> List[Dict]:
        """å¢å¼ºæ£€ç´¢ç»“æœçš„å¤šæ ·æ€§
        
        ä½¿ç”¨MMR(Maximum Marginal Relevance)ç®—æ³•å¹³è¡¡ç›¸å…³æ€§å’Œå¤šæ ·æ€§
        
        :param ranked_results: æŒ‰åˆ†æ•°æ’åºçš„æ£€ç´¢ç»“æœ
        :return: å¤šæ ·æ€§å¢å¼ºåçš„ç»“æœ
        """
        if len(ranked_results) <= 2:
            return ranked_results  # ç»“æœå¤ªå°‘ä¸éœ€è¦å¤šæ ·æ€§ä¼˜åŒ–
        
        try:
            # MMRå‚æ•°
            lambda_param = 0.7  # æ§åˆ¶ç›¸å…³æ€§vså¤šæ ·æ€§çš„å¹³è¡¡ï¼Œè¶Šå¤§è¶Šåå‘ç›¸å…³æ€§
            
            # åˆå§‹åŒ–å·²é€‰æ‹©å’Œå€™é€‰æ–‡æ¡£
            selected = [ranked_results[0]]  # æœ€é«˜åˆ†æ–‡æ¡£ç›´æ¥é€‰å…¥
            candidates = ranked_results[1:]
            
            while len(selected) < min(len(ranked_results), self.config.final_top_k):
                # è®¡ç®—æ¯ä¸ªå€™é€‰æ–‡æ¡£çš„MMRåˆ†æ•°
                mmr_scores = []
                
                for candidate in candidates:
                    # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆç›¸å…³æ€§éƒ¨åˆ†ï¼‰
                    relevance = candidate["final_score"]
                    
                    # è®¡ç®—ä¸å·²é€‰æ–‡æ¡£çš„æœ€å¤§ç›¸ä¼¼åº¦ï¼ˆå¤šæ ·æ€§éƒ¨åˆ†ï¼‰
                    max_sim = 0
                    for selected_doc in selected:
                        # ä½¿ç”¨æ–‡æœ¬å†…å®¹çš„è¯é‡å è®¡ç®—ç›¸ä¼¼åº¦
                        sim = self._compute_document_similarity(
                            candidate["doc"].page_content,
                            selected_doc["doc"].page_content
                        )
                        max_sim = max(max_sim, sim)
                    
                    # è®¡ç®—MMRåˆ†æ•°
                    mmr = lambda_param * relevance - (1 - lambda_param) * max_sim
                    mmr_scores.append(mmr)
                
                # é€‰æ‹©MMRåˆ†æ•°æœ€é«˜çš„æ–‡æ¡£
                best_idx = mmr_scores.index(max(mmr_scores))
                selected.append(candidates.pop(best_idx))
            
            # è¿”å›å¤šæ ·æ€§å¢å¼ºåçš„æ–‡æ¡£
            return selected
            
        except Exception as e:
            logger.error(f"å¤šæ ·æ€§å¢å¼ºå¤±è´¥: {str(e)}")
            # å¤±è´¥æ—¶è¿”å›åŸå§‹æ’åº
            return ranked_results[:self.config.final_top_k]
    
    def _compute_document_similarity(self, doc1: str, doc2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼åº¦
        
        :param doc1: ç¬¬ä¸€ä¸ªæ–‡æ¡£å†…å®¹
        :param doc2: ç¬¬äºŒä¸ªæ–‡æ¡£å†…å®¹
        :return: ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰
        """
        try:
            # ä½¿ç”¨åŸºäºè¯é›†åˆçš„Jaccardç›¸ä¼¼åº¦
            tokens1 = set(self._tokenize(doc1))
            tokens2 = set(self._tokenize(doc2))
            
            # è®¡ç®—Jaccardç³»æ•°
            if not tokens1 or not tokens2:
                return 0.0
                
            intersection = tokens1.intersection(tokens2)
            union = tokens1.union(tokens2)
            
            # å¦‚æœæ–‡æ¡£é•¿åº¦ç›¸å·®å¤ªå¤§ï¼Œç»™äºˆæƒ©ç½š
            len_ratio = min(len(doc1), len(doc2)) / max(len(doc1), len(doc2))
            
            # åŠ æƒç›¸ä¼¼åº¦
            return (len(intersection) / len(union)) * len_ratio
            
        except Exception as e:
            logger.warning(f"æ–‡æ¡£ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {str(e)}")
            return 0.0

    def _retrieve_documents(self, question: str) -> Tuple[List[Document], List[Dict]]:
        """å®Œæ•´æ£€ç´¢æµç¨‹

        :param question: ç”¨æˆ·é—®é¢˜
        :return: (æ–‡æ¡£åˆ—è¡¨, åˆ†æ•°ä¿¡æ¯åˆ—è¡¨)
        """
        try:
            # æ··åˆæ£€ç´¢
            raw_results = self._hybrid_retrieve(question)
            if not raw_results:
                return [], []

            # åˆ†æ•°å½’ä¸€åŒ–
            norm_results = self._normalize_scores(raw_results)

            # é‡æ’åº
            reranked = self._rerank_documents(norm_results, question)

            # æ ¹æ®é˜ˆå€¼è¿‡æ»¤ç»“æœ
            filtered = [
                res for res in reranked
                if res["final_score"] >= self.config.similarity_threshold
            ]
            final_results = sorted(
                filtered,
                key=lambda x: x["final_score"],
                reverse=True
            )[:self.config.final_top_k]
            # æå–æ–‡æ¡£å’Œåˆ†æ•°ä¿¡æ¯
            docs = [res["doc"] for res in final_results]
            score_info = [{
                "source": res["source"],
                "type": res["type"],
                "vector_score": res.get("score", 0),  # å…¼å®¹ä¸åŒæ£€ç´¢ç±»å‹
                "bm25_score": res.get("score", 0),
                "rerank_score": res["rerank_score"],
                "final_score": res["final_score"]
            } for res in final_results]

            return docs, score_info
        except Exception as e:
            logger.error(f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {str(e)}")
            raise

    def _build_prompt(self, question: str, context: str) -> str:
        """æ„å»ºç¬¦åˆdeepseek-r1æ ¼å¼çš„æç¤ºæ¨¡æ¿

        :param question: ç”¨æˆ·é—®é¢˜
        :param context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
        :return: æ ¼å¼åŒ–åçš„æç¤ºå­—ç¬¦ä¸²
        """
        if context:
            return (
                "<|im_start|>system\n"
                "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„åŒ–å·¥å®‰å…¨é¢†åŸŸä¸“å®¶...\n"
                "ä¸Šä¸‹æ–‡ï¼š\n{context}\n"
                "<|im_end|>\n"
                "<|im_start|>user\n"
                "{question}\n"
                "<|im_end|>\n"
                "<|im_start|>assistant\n"
            ).format(question=question, context=context[:self.config.max_context_length])
        else:
            return (
                "<|im_start|>user\n"
                "{question}\n"
                "<|im_end|>\n"
                "<|im_start|>assistant\n"
            ).format(question=question)

    def _format_references(self, docs: List[Document], score_info: List[Dict]) -> List[Dict]:
        """æ ¼å¼åŒ–å‚è€ƒæ–‡æ¡£ä¿¡æ¯"""
        return [
            {
                "file": str(Path(info["source"]).name),  # æ–‡ä»¶å
                "content": doc.page_content,  # æˆªå–å‰500å­—ç¬¦
                "score": info["final_score"],  # ç»¼åˆè¯„åˆ†
                "type": info["type"],  # æ£€ç´¢ç±»å‹
                "full_path": info["source"]  # å®Œæ•´æ–‡ä»¶è·¯å¾„
            }
            for doc, info in zip(docs, score_info)
        ]
    def stream_query_model(self, question: str) -> Generator[str, None, None]:
        """çº¯æ¨¡å‹æµå¼ç”Ÿæˆï¼ˆä¸ç»è¿‡RAGï¼‰"""
        logger.info(f"ğŸŒ€ æ­£åœ¨ç›´æ¥æµå¼ç”Ÿæˆ: {question[:50]}...")
        try:
            if not question.strip():
                yield "âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜"
                return

            # æ„å»ºåŸºç¡€æç¤ºæ¨¡æ¿ï¼ˆä¸åŒ…å«ä¸Šä¸‹æ–‡ï¼‰
            prompt = (
                "<|im_start|>system\n"
                "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„åŒ–å·¥å®‰å…¨é¢†åŸŸä¸“å®¶ï¼Œè¯·ä¸“ä¸šä¸”å‡†ç¡®åœ°å›ç­”é—®é¢˜ã€‚\n"
                "<|im_end|>\n"
                "<|im_start|>user\n"
                f"{question}\n"
                "<|im_end|>\n"
                "<|im_start|>assistant\n"
            )

            try:
                full_response = ""
                for chunk in self.llm.stream(prompt):
                    cleaned_chunk = chunk.replace("<|im_end|>", "")
                    if cleaned_chunk:
                        # å‘é€ç”Ÿæˆå†…å®¹ï¼ˆä½œä¸ºæ™®é€šæ–‡æœ¬ï¼‰
                        yield json.dumps({
                            "type": "content",
                            "data": cleaned_chunk
                        }) + "\n"

            except Exception as e:
                logger.error(f"ç›´æ¥ç”Ÿæˆä¸­æ–­: {str(e)}")
                yield "\nâš ï¸ ç”Ÿæˆè¿‡ç¨‹å‘ç”Ÿæ„å¤–ä¸­æ–­ï¼Œè¯·ç¨åé‡è¯•"

        except Exception as e:
            logger.exception("ç›´æ¥æµå¼ç”Ÿæˆé”™è¯¯")
            yield "âš ï¸ ç³»ç»Ÿå¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯ï¼Œè¯·è”ç³»ç®¡ç†å‘˜"

    def stream_query_rag_with_kb(self, question: str) -> Generator[str, None, None]:
        """ç»“åˆçŸ¥è¯†åº“çš„æµå¼RAGç”Ÿæˆ"""
        logger.info(f"ğŸŒŠ æ­£åœ¨æµå¼å¤„ç†æŸ¥è¯¢: {question[:50]}...")
        if not question.strip():
            yield "âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜"
            return

        try:
            # é˜¶æ®µ1ï¼šæ–‡æ¡£æ£€ç´¢
            try:
                docs, score_info = self._retrieve_documents(question)
                if not docs:
                    yield "âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£..."
                    return
            except Exception as e:
                logger.error(f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {str(e)}")
                yield "âš ï¸ æ–‡æ¡£æ£€ç´¢æœåŠ¡æš‚æ—¶ä¸å¯ç”¨"
                return

            # æ ¼å¼åŒ–å‚è€ƒæ–‡æ¡£ä¿¡æ¯
            references = self._format_references(docs, score_info)

            # å‘é€å‚è€ƒæ–‡æ¡£ä¿¡æ¯
            yield json.dumps({
                "type": "references",
                "data": references
            }) + "\n"  # æ·»åŠ æ¢è¡Œç¬¦ä½œä¸ºç»“æŸæ ‡è®°

            # é˜¶æ®µ2ï¼šæ„å»ºä¸Šä¸‹æ–‡
            context = "\n\n".join([
                f"ã€å‚è€ƒæ–‡æ¡£{i + 1}ã€‘{doc.page_content}\n"
                f"- æ¥æº: {Path(info['source']).name}\n"
                f"- ç»¼åˆç½®ä¿¡åº¦: {info['final_score'] * 100:.1f}%"
                for i, (doc, info) in enumerate(zip(docs, score_info))
            ])

            # é˜¶æ®µ3ï¼šæ„å»ºæç¤ºæ¨¡æ¿
            prompt = self._build_prompt(question, context)

            # é˜¶æ®µ4ï¼šæµå¼ç”Ÿæˆ
            try:
                full_response = ""
                for chunk in self.llm.stream(prompt):
                    cleaned_chunk = chunk.replace("<|im_end|>", "")
                    if cleaned_chunk:
                        # å‘é€ç”Ÿæˆå†…å®¹
                        yield json.dumps({
                            "type": "content",
                            "data": cleaned_chunk
                        }) + "\n"

            except Exception as e:
                logger.error(f"æµå¼ç”Ÿæˆä¸­æ–­: {str(e)}")
                yield json.dumps({
                    "type": "error",
                    "data": "\nâš ï¸ ç”Ÿæˆè¿‡ç¨‹å‘ç”Ÿæ„å¤–ä¸­æ–­"
                }) + "\n"

        except Exception as e:
            logger.exception("æµå¼å¤„ç†ä¸¥é‡é”™è¯¯")
            yield json.dumps({
                "type": "error",
                "data": "âš ï¸ ç³»ç»Ÿå¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯"
            }) + "\n"

    def answer_query(self, question: str) -> Tuple[str, List[Dict], Dict]:
        """éæµå¼RAGç”Ÿæˆï¼Œé€‚ç”¨äºè¯„ä¼°æ¨¡å—
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            Tuple(ç”Ÿæˆçš„å›ç­”, æ£€ç´¢çš„æ–‡æ¡£åˆ—è¡¨, å…ƒæ•°æ®)
        """
        logger.info(f"ğŸ” éæµå¼å¤„ç†æŸ¥è¯¢(ç”¨äºè¯„ä¼°): {question[:50]}...")
        
        try:
            # é˜¶æ®µ1ï¼šæ–‡æ¡£æ£€ç´¢
            docs, score_info = self._retrieve_documents(question)
            if not docs:
                return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œæ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚", [], {"status": "no_docs"}
            
            # æ ¼å¼åŒ–å‚è€ƒæ–‡æ¡£ä¿¡æ¯
            references = self._format_references(docs, score_info)
            
            # é˜¶æ®µ2ï¼šæ„å»ºä¸Šä¸‹æ–‡
            context = "\n\n".join([
                f"ã€å‚è€ƒæ–‡æ¡£{i + 1}ã€‘{doc.page_content}\n"
                f"- æ¥æº: {Path(info['source']).name}\n"
                f"- ç»¼åˆç½®ä¿¡åº¦: {info['final_score'] * 100:.1f}%"
                for i, (doc, info) in enumerate(zip(docs, score_info))
            ])
            
            # é˜¶æ®µ3ï¼šæ„å»ºæç¤ºæ¨¡æ¿
            prompt = self._build_prompt(question, context)
            
            # é˜¶æ®µ4ï¼šä¸€æ¬¡æ€§ç”Ÿæˆï¼ˆéæµå¼ï¼‰
            answer = self.llm.invoke(prompt)
            cleaned_answer = answer.replace("<|im_end|>", "").strip()
            
            return cleaned_answer, references, {"status": "success"}
            
        except Exception as e:
            logger.exception(f"éæµå¼å¤„ç†ä¸¥é‡é”™è¯¯: {str(e)}")
            return f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", [], {"status": "error", "error": str(e)}

