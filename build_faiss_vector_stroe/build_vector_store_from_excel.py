import os
import pandas as pd
from pathlib import Path
import logging
import sys
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.vectorstores.utils import DistanceStrategy
from config import Config
import shutil
from datetime import datetime
import jieba
from typing import List, Dict, Any
import hashlib

# é…ç½®æ—¥å¿—æ ¼å¼
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(stream=sys.stdout)],
    force=True
)
logger = logging.getLogger(__name__)

class ExcelVectorDBBuilder:
    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–Excelå‘é‡æ•°æ®åº“æ„å»ºå™¨
        Args:
            config (Config): é…ç½®ç±»ï¼ŒåŒ…å«å¿…è¦çš„é…ç½®
        """
        self.config = config
        
        # è®¾ç½®å‘é‡æ•°æ®åº“è·¯å¾„
        self.vector_dir = Path(config.vector_db_path)
        self.vector_backup_dir = self.vector_dir / "backups"
        
        # è®¾ç½®Excelæ–‡ä»¶ç›®å½•
        self.excel_dir = self.config.excel_dir
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.vector_dir.mkdir(parents=True, exist_ok=True)
        self.excel_dir.mkdir(parents=True, exist_ok=True)

    def _print_chunk_preview(self, content: str, source: str, index: int, total: int):
        """æ‰“å°æ–‡æœ¬å—é¢„è§ˆ
        Args:
            content: æ–‡æœ¬å†…å®¹
            source: æ¥æºæ–‡ä»¶å
            index: å½“å‰å—ç´¢å¼•
            total: æ€»å—æ•°
        """

            
        logger.info("\n" + "="*50)
        logger.info(f"ğŸ“„ æ–‡ä»¶: {source}")
        logger.info(f"ğŸ“‘ å— {index + 1}/{total}")
        logger.info("-"*50)
        logger.info(f"å†…å®¹é¢„è§ˆ ({len(content)} å­—ç¬¦):")
        logger.info(content)
        logger.info("="*50 + "\n")

    def load_chunks_from_excel(self) -> List[Document]:
        """ä»Excelæ–‡ä»¶ä¸­åŠ è½½æ–‡æœ¬å—"""
        logger.info("å¼€å§‹ä»Excelæ–‡ä»¶åŠ è½½æ–‡æœ¬å—...")
        
        all_chunks = []
        excel_files = list(self.excel_dir.glob("*.xlsx"))
        
        if not excel_files:
            logger.warning(f"åœ¨ {self.excel_dir} ç›®å½•ä¸‹æœªæ‰¾åˆ°Excelæ–‡ä»¶")
            return []
            
        for excel_file in excel_files:
            try:
                # è¯»å–Excelæ–‡ä»¶
                df = pd.read_excel(excel_file)
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„åˆ—
                if "å…¥åº“å†…å®¹" not in df.columns:
                    logger.warning(f"Excelæ–‡ä»¶ {excel_file.name} ä¸­ç¼ºå°‘'å…¥åº“å†…å®¹'åˆ—")
                    continue
                
                # è·å–æ–‡ä»¶åä½œä¸ºæ¥æº
                source = excel_file.name
                logger.info(f"\nğŸ“‚ æ­£åœ¨å¤„ç†æ–‡ä»¶: {source}")
                
                # å¤„ç†æ¯ä¸ªæ–‡æœ¬å—
                for idx, row in df.iterrows():
                    content = str(row["å…¥åº“å†…å®¹"]).strip()
                    if not content:  # è·³è¿‡ç©ºå†…å®¹
                        continue
                        
                    # å¯¹å†…å®¹è¿›è¡Œåˆ†è¯å¤„ç†
                    tokenized_content = self._tokenize(content)
                    
                    # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
                    doc = Document(
                        page_content=content,
                        metadata={
                            "source": str(excel_file),
                            "file_name": source,
                            "chunk_index": idx,
                            "total_chunks": len(df),
                            "tokens": tokenized_content,
                            "token_count": len(tokenized_content),
                            "content_hash": hashlib.md5(content.encode()).hexdigest(),
                            "processed_at": datetime.now().isoformat()
                        }
                    )
                    all_chunks.append(doc)
                    
                    # æ‰“å°æ–‡æœ¬å—é¢„è§ˆ
                    self._print_chunk_preview(content, source, idx, len(df))
                    
                logger.info(f"âœ… ä» {source} æˆåŠŸåŠ è½½ {len(df)} ä¸ªæ–‡æœ¬å—")
                
            except Exception as e:
                logger.error(f"å¤„ç†Excelæ–‡ä»¶ {excel_file.name} æ—¶å‡ºé”™: {str(e)}")
                continue
                
        logger.info(f"âœ… æ€»å…±åŠ è½½äº† {len(all_chunks)} ä¸ªæ–‡æœ¬å—")
        return all_chunks

    def _tokenize(self, text: str) -> List[str]:
        """ä¸“ä¸šä¸­æ–‡åˆ†è¯å¤„ç†
        :param text: å¾…åˆ†è¯çš„æ–‡æœ¬
        :return: åˆ†è¯åçš„è¯é¡¹åˆ—è¡¨
        """
        return [word for word in jieba.cut(text) if word.strip()]

    def backup_vector_db(self):
        """å¤‡ä»½ç°æœ‰å‘é‡æ•°æ®åº“"""
        if not self.vector_dir.exists():
            return False
            
        try:
            # åˆ›å»ºå¤‡ä»½ç›®å½•
            backup_dir = self.vector_dir.parent / f"{self.vector_dir.name}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            backup_dir.mkdir(parents=True, exist_ok=True)
            
            # å¤åˆ¶æ‰€æœ‰æ–‡ä»¶åˆ°å¤‡ä»½ç›®å½•
            for item in self.vector_dir.glob('*'):
                if item.is_file():
                    shutil.copy2(item, backup_dir)
                elif item.is_dir():
                    shutil.copytree(item, backup_dir / item.name)
                    
            logger.info(f"âœ… å‘é‡æ•°æ®åº“å·²å¤‡ä»½è‡³ {backup_dir}")
            return True
        except Exception as e:
            logger.error(f"å¤‡ä»½å‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
            return False

    def create_embeddings(self) -> HuggingFaceEmbeddings:
        """åˆ›å»ºåµŒå…¥æ¨¡å‹å®ä¾‹"""
        logger.info("åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
        return HuggingFaceEmbeddings(
            model_name=self.config.embedding_model_path,
            model_kwargs={"device": self.config.device},
            encode_kwargs={
                "batch_size": self.config.batch_size,
                "normalize_embeddings": self.config.normalize_embeddings
            },
        )

    def build_vector_store(self):
        """æ„å»ºå‘é‡æ•°æ®åº“"""
        logger.info("å¼€å§‹æ„å»ºå‘é‡æ•°æ®åº“")
        
        # åŠ è½½æ–‡æœ¬å—
        chunks = self.load_chunks_from_excel()
        
        if not chunks:
            logger.warning("æ²¡æœ‰æ–‡æœ¬å—å¯ä»¥å¤„ç†ï¼Œè·³è¿‡å‘é‡å­˜å‚¨æ„å»º")
            return
            
        # å¦‚æœå‘é‡æ•°æ®åº“å·²å­˜åœ¨ï¼Œå…ˆå¤‡ä»½
        if self.vector_dir.exists() and any(self.vector_dir.glob('*')):
            self.backup_vector_db()
            
        # ç”ŸæˆåµŒå…¥æ¨¡å‹
        embeddings = self.create_embeddings()
        
        # æ„å»ºå‘é‡å­˜å‚¨
        logger.info("ç”Ÿæˆå‘é‡...")
        vector_store = FAISS.from_documents(
            chunks,
            embeddings,
            distance_strategy=DistanceStrategy.COSINE
        )
        
        # ä¿å­˜å‘é‡æ•°æ®åº“
        vector_store.save_local(str(self.vector_dir))
        logger.info(f"âœ… å‘é‡æ•°æ®åº“å·²ä¿å­˜è‡³ {self.vector_dir}")

if __name__ == "__main__":
    try:
        # åˆå§‹åŒ–é…ç½®
        config = Config()
        
        # æ„å»ºå‘é‡æ•°æ®åº“
        builder = ExcelVectorDBBuilder(config)
        builder.build_vector_store()
        
    except Exception as e:
        logger.exception("ç¨‹åºè¿è¡Œå‡ºé”™")
    finally:
        logger.info("ç¨‹åºè¿è¡Œç»“æŸ") 